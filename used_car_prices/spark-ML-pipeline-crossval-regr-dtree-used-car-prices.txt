
val df = spark.read.format("csv").option("inferSchema","true").load("spark/used_cars/used_cars_price_train-data.csv").
         toDF("Row","Name","Location","Year","Kilometers_Driven","Fuel_Type","Transmission","Owner_Type","Mileage","Engine","Power","Seats","New_Price","Price")

df.printSchema
root
 |-- Row: integer (nullable = true)
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = true)
 |-- Engine: string (nullable = true)
 |-- Power: string (nullable = true)
 |-- Seats: double (nullable = true)
 |-- New_Price: string (nullable = true)
 |-- Price: double (nullable = true)

df.describe().show
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+
|summary|               Row|                Name| Location|              Year|Kilometers_Driven|Fuel_Type|Transmission|Owner_Type| Mileage| Engine|   Power|             Seats| New_Price|             Price|
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+
|  count|              6019|                6019|     6019|              6019|             6019|     6019|        6019|      6019|    6017|   5983|    5983|              5977|       824|              6019|
|   mean|            3009.0|                null|     null|2013.3581990363848|58738.38029573019|     null|        null|      null|    null|   null|    null| 5.278735151413753|      null| 9.479468350224273|
| stddev|1737.6799666988932|                null|     null|  3.26974211609139|91268.84320624865|     null|        null|      null|    null|   null|    null|0.8088395547482933|      null|11.187917112455484|
|    min|                 0|Ambassador Classi...|Ahmedabad|              1998|              171|      CNG|   Automatic|     First|0.0 kmpl|1047 CC| 100 bhp|               0.0|      1 Cr|              0.44|
|    max|              6018|Volvo XC90 2007-2...|     Pune|              2019|          6500000|   Petrol|      Manual|     Third|9.9 kmpl| 999 CC|null bhp|              10.0|99.92 Lakh|             160.0|
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+

df.groupBy("Mileage").count.orderBy('count.desc).show(3)
+---------+-----+
|  Mileage|count|
+---------+-----+
|17.0 kmpl|  172|
|18.9 kmpl|  172|
|18.6 kmpl|  119|
+---------+-----+
only showing top 3 rows

df.groupBy("Engine").count.orderBy('count.desc).show(3)
+-------+-----+
| Engine|count|
+-------+-----+
|1197 CC|  606|
|1248 CC|  512|
|1498 CC|  304|
+-------+-----+
only showing top 3 rows

df.groupBy("Power").count.orderBy('count.desc).show(3)
+--------+-----+
|   Power|count|
+--------+-----+
|  74 bhp|  235|
|98.6 bhp|  131|
|73.9 bhp|  125|
+--------+-----+
only showing top 3 rows

df.groupBy("Seats").count.orderBy('count.desc).show(3)
+-----+-----+
|Seats|count|
+-----+-----+
|  5.0| 5014|
|  7.0|  674|
|  8.0|  134|
+-----+-----+
only showing top 3 rows


val df1 = df.na.fill(Map("Mileage" -> "18.9 kmpl", "Engine" -> "1197 CC", "Power" -> "74 bhp", "Seats" -> 5.0)).
          na.replace("Power", Map("null bhp" -> "74 bhp"))

import org.apache.spark.sql.types._

val df2 = df1.withColumn("age", (lit(2021)-'Year).cast(DoubleType)).
          withColumn("mileage_aux", regexp_extract('Mileage,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumn("engine_aux", regexp_extract('Engine,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumn("power_aux", regexp_extract('Power,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumnRenamed("Price", "label").
          drop("Row")

df2.printSchema
root
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = false)
 |-- Engine: string (nullable = false)
 |-- Power: string (nullable = false)
 |-- Seats: double (nullable = false)
 |-- New_Price: string (nullable = true)
 |-- label: double (nullable = true)
 |-- age: double (nullable = true)
 |-- mileage_aux: double (nullable = true)
 |-- engine_aux: double (nullable = true)
 |-- power_aux: double (nullable = true)

df2.describe().show
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+
|summary|                Name| Location|              Year|Kilometers_Driven|Fuel_Type|Transmission|Owner_Type| Mileage| Engine|   Power|             Seats| New_Price|             label|               age|       mileage_aux|        engine_aux|        power_aux|
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+
|  count|                6019|     6019|              6019|             6019|     6019|        6019|      6019|    6019|   6019|    6019|              6019|       824|              6019|              6019|              6019|              6019|             6019|
|   mean|                null|     null|2013.3581990363848|58738.38029573019|     null|        null|      null|    null|   null|    null|5.2767901644791495|      null| 9.479468350224273| 7.641800963615219|18.135215152018656|1618.7388270476824|112.3204718391751|
| stddev|                null|     null|  3.26974211609139|91268.84320624865|     null|        null|      null|    null|   null|    null|0.8063460892297473|      null|11.187917112455484|3.2697421160913938| 4.581548857057788| 600.4458584135865|53.56569974837497|
|    min|Ambassador Classi...|Ahmedabad|              1998|              171|      CNG|   Automatic|     First|0.0 kmpl|1047 CC| 100 bhp|               0.0|      1 Cr|              0.44|               2.0|               0.0|              72.0|             34.2|
|    max|Volvo XC90 2007-2...|     Pune|              2019|          6500000|   Petrol|      Manual|     Third|9.9 kmpl| 999 CC|99.6 bhp|              10.0|99.92 Lakh|             160.0|              23.0|             33.54|            5998.0|            560.0|
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+

df2.groupBy("Location").count.show
+----------+-----+
|  Location|count|
+----------+-----+
| Bangalore|  358|
|     Kochi|  651|
|   Chennai|  494|
|    Mumbai|  790|
| Ahmedabad|  224|
|   Kolkata|  535|
|      Pune|  622|
|     Delhi|  554|
|Coimbatore|  636|
| Hyderabad|  742|
|    Jaipur|  413|
+----------+-----+

df2.groupBy("Fuel_Type").count.show
+---------+-----+
|Fuel_Type|count|
+---------+-----+
|   Diesel| 3205|
|      CNG|   56|
| Electric|    2|
|      LPG|   10|
|   Petrol| 2746|
+---------+-----+

df2.groupBy("Transmission").count.show
+------------+-----+
|Transmission|count|
+------------+-----+
|   Automatic| 1720|
|      Manual| 4299|
+------------+-----+

df2.groupBy("Owner_Type").count.show
+--------------+-----+
|    Owner_Type|count|
+--------------+-----+
|         First| 4929|
|        Second|  968|
|Fourth & Above|    9|
|         Third|  113|
+--------------+-----+


import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("Location").setOutputCol("LocationCat").setHandleInvalid("skip")
val dfInd2 = new StringIndexer().setInputCol("Fuel_Type").setOutputCol("Fuel_TypeCat").setHandleInvalid("skip")
val dfInd3 = new StringIndexer().setInputCol("Transmission").setOutputCol("TransmissionCat").setHandleInvalid("skip")
val dfInd4 = new StringIndexer().setInputCol("Owner_Type").setOutputCol("Owner_TypeCat").setHandleInvalid("skip")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("Kilometers_Driven","Seats","age","mileage_aux","engine_aux","power_aux", "Fuel_TypeCat","TransmissionCat","Owner_TypeCat","LocationCat"))

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,va))

val df3 = pipeline.fit(df2).transform(df2)

df3.printSchema
root
 |-- city: string (nullable = true)
 |-- zone: string (nullable = true)
 |-- bathrooms: long (nullable = true)
 |-- bedrooms: long (nullable = true)
 |-- floors: long (nullable = true)
 |-- parkingSpaces: long (nullable = true)
 |-- suites: long (nullable = true)
 |-- totalAreas: string (nullable = true)
 |-- unitFloor: long (nullable = true)
 |-- unitsOnTheFloor: long (nullable = true)
 |-- usableAreas: double (nullable = true)
 |-- price: string (nullable = true)
 |-- unit: string (nullable = true)
 |-- usage: string (nullable = true)
 |-- label: double (nullable = true)
 |-- unitIdx: double (nullable = false)
 |-- usageIdx: double (nullable = false)
 |-- zoneIdx: double (nullable = false)
 |-- features: vector (nullable = true)

 
df3.select("label","features").show(10,false)
+-------+----------------------------------------------+
|label  |features                                      |
+-------+----------------------------------------------+
|45000.0|(11,[4,8,9],[1.0,62.0,2.0])                   |
|45000.0|(11,[1,2,4,8],[1.0,2.0,1.0,44.0])             |
|50000.0|(11,[8,9],[132.0,2.0])                        |
|45000.0|(11,[0,1,6,8,9,10],[2.0,1.0,3.0,32.0,2.0,1.0])|
|50000.0|[0.0,2.0,3.0,3.0,1.0,1.0,2.0,4.0,60.0,0.0,0.0]|
|50000.0|(11,[8,9],[200.0,2.0])                        |
|50000.0|(11,[1,6,8,9,10],[1.0,2.0,25.0,2.0,1.0])      |
|50000.0|(11,[0,1,8,9,10],[2.0,1.0,23.0,2.0,1.0])      |
|45000.0|(11,[0,8,9,10],[2.0,60.0,2.0,1.0])            |
|45336.0|(11,[0,1,2,4,8],[2.0,1.0,1.0,1.0,33.0])       |
+-------+----------------------------------------------+
only showing top 10 rows


// ----- building the decision tree model

import org.apache.spark.ml.regression.DecisionTreeRegressor
val dt = new DecisionTreeRegressor
dt.setFeaturesCol("features")

import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,va,dt))

val Array(trainingData, testData) = df2.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

// ----- find best decision tree model

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(dt.maxDepth, Array(7, 10, 20)).
addGrid(dt.maxBins, Array(16, 32, 48)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.DecisionTreeRegressionModel
val dtmodel = bestmodel.stages(5).asInstanceOf[DecisionTreeRegressionModel]

// -----  metrics extracted from model

dtmodel.getMaxDepth
res9: Int = 7

dtmodel.getMaxBins
res10: Int = 16

val pred = bestmodel.transform(trainingData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res25: Double = 3.5127417236677756

bceval.setMetricName("r2").evaluate(pred)
res26: Double = 0.8972308677585834

dtmodel.toDebugString
res27: String =
"DecisionTreeRegressionModel (uid=dtr_3112f4fa5043) of depth 7 with 233 nodes
  If (feature 5 <= 157.775)
   If (feature 5 <= 120.35)
    If (feature 2 <= 7.5)
     If (feature 5 <= 100.3)
      If (feature 5 <= 73.92)
       If (feature 6 in {1.0,2.0})
        If (feature 4 <= 996.5)
         Predict: 3.0437500000000006
        Else (feature 4 > 996.5)
         Predict: 3.9672151898734183
       Else (feature 6 not in {1.0,2.0})
        If (feature 7 in {0.0})
         Predict: 4.628846153846155
        Else (feature 7 not in {0.0})
         Predict: 9.879999999999995
      Else (feature 5 > 73.92)
       If (feature 2 <= 5.5)
        If (feature 4 <= 1220.5)
         Predict: 6.158659003831419
        Else (feature 4 > 1220.5)
         Predict: 7.5499206349206345
     ...
	 
// -----  metrics on test data

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res28: Double = 5.260083237075163

bceval.setMetricName("r2").evaluate(pred)
res29: Double = 0.7990681476198392