
val df = spark.read.format("csv").option("inferSchema","true").load("spark/used_cars/used_cars_price_train-data.csv").
         toDF("Row","Name","Location","Year","Kilometers_Driven","Fuel_Type","Transmission","Owner_Type","Mileage","Engine","Power","Seats","New_Price","Price")

df.printSchema
root
 |-- Row: integer (nullable = true)
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = true)
 |-- Engine: string (nullable = true)
 |-- Power: string (nullable = true)
 |-- Seats: double (nullable = true)
 |-- New_Price: string (nullable = true)
 |-- Price: double (nullable = true)

df.describe().show
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+
|summary|               Row|                Name| Location|              Year|Kilometers_Driven|Fuel_Type|Transmission|Owner_Type| Mileage| Engine|   Power|             Seats| New_Price|             Price|
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+
|  count|              6019|                6019|     6019|              6019|             6019|     6019|        6019|      6019|    6017|   5983|    5983|              5977|       824|              6019|
|   mean|            3009.0|                null|     null|2013.3581990363848|58738.38029573019|     null|        null|      null|    null|   null|    null| 5.278735151413753|      null| 9.479468350224273|
| stddev|1737.6799666988932|                null|     null|  3.26974211609139|91268.84320624865|     null|        null|      null|    null|   null|    null|0.8088395547482933|      null|11.187917112455484|
|    min|                 0|Ambassador Classi...|Ahmedabad|              1998|              171|      CNG|   Automatic|     First|0.0 kmpl|1047 CC| 100 bhp|               0.0|      1 Cr|              0.44|
|    max|              6018|Volvo XC90 2007-2...|     Pune|              2019|          6500000|   Petrol|      Manual|     Third|9.9 kmpl| 999 CC|null bhp|              10.0|99.92 Lakh|             160.0|
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+

df.groupBy("Mileage").count.orderBy('count.desc).show(3)
+---------+-----+
|  Mileage|count|
+---------+-----+
|17.0 kmpl|  172|
|18.9 kmpl|  172|
|18.6 kmpl|  119|
+---------+-----+
only showing top 3 rows

df.groupBy("Engine").count.orderBy('count.desc).show(3)
+-------+-----+
| Engine|count|
+-------+-----+
|1197 CC|  606|
|1248 CC|  512|
|1498 CC|  304|
+-------+-----+
only showing top 3 rows

df.groupBy("Power").count.orderBy('count.desc).show(3)
+--------+-----+
|   Power|count|
+--------+-----+
|  74 bhp|  235|
|98.6 bhp|  131|
|73.9 bhp|  125|
+--------+-----+
only showing top 3 rows

df.groupBy("Seats").count.orderBy('count.desc).show(3)
+-----+-----+
|Seats|count|
+-----+-----+
|  5.0| 5014|
|  7.0|  674|
|  8.0|  134|
+-----+-----+
only showing top 3 rows


val df1 = df.na.fill(Map("Mileage" -> "18.9 kmpl", "Engine" -> "1197 CC", "Power" -> "74 bhp", "Seats" -> 5.0)).
          na.replace("Power", Map("null bhp" -> "74 bhp"))

import org.apache.spark.sql.types._

val df2 = df1.withColumn("age", (lit(2021)-'Year).cast(DoubleType)).
          withColumn("mileage_aux", regexp_extract('Mileage,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumn("engine_aux", regexp_extract('Engine,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumn("power_aux", regexp_extract('Power,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
		  withColumn("Kilometers_Driven", 'Kilometers_Driven / 1000).
          withColumnRenamed("Price", "label").
          drop("Row")

df2.printSchema
root
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = false)
 |-- Engine: string (nullable = false)
 |-- Power: string (nullable = false)
 |-- Seats: double (nullable = false)
 |-- New_Price: string (nullable = true)
 |-- label: double (nullable = true)
 |-- age: double (nullable = true)
 |-- mileage_aux: double (nullable = true)
 |-- engine_aux: double (nullable = true)
 |-- power_aux: double (nullable = true)

df2.describe().show
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+
|summary|                Name| Location|              Year|Kilometers_Driven|Fuel_Type|Transmission|Owner_Type| Mileage| Engine|   Power|             Seats| New_Price|             label|               age|       mileage_aux|        engine_aux|        power_aux|
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+
|  count|                6019|     6019|              6019|             6019|     6019|        6019|      6019|    6019|   6019|    6019|              6019|       824|              6019|              6019|              6019|              6019|             6019|
|   mean|                null|     null|2013.3581990363848|58.73838029573025|     null|        null|      null|    null|   null|    null|5.2767901644791495|      null| 9.479468350224273| 7.641800963615219|18.135215152018656|1618.7388270476824|112.3204718391751|
| stddev|                null|     null|  3.26974211609139| 91.2688432062487|     null|        null|      null|    null|   null|    null|0.8063460892297473|      null|11.187917112455484|3.2697421160913938| 4.581548857057788| 600.4458584135865|53.56569974837497|
|    min|Ambassador Classi...|Ahmedabad|              1998|            0.171|      CNG|   Automatic|     First|0.0 kmpl|1047 CC| 100 bhp|               0.0|      1 Cr|              0.44|               2.0|               0.0|              72.0|             34.2|
|    max|Volvo XC90 2007-2...|     Pune|              2019|           6500.0|   Petrol|      Manual|     Third|9.9 kmpl| 999 CC|99.6 bhp|              10.0|99.92 Lakh|             160.0|              23.0|             33.54|            5998.0|            560.0|
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+

df2.groupBy("Location").count.show
+----------+-----+
|  Location|count|
+----------+-----+
| Bangalore|  358|
|     Kochi|  651|
|   Chennai|  494|
|    Mumbai|  790|
| Ahmedabad|  224|
|   Kolkata|  535|
|      Pune|  622|
|     Delhi|  554|
|Coimbatore|  636|
| Hyderabad|  742|
|    Jaipur|  413|
+----------+-----+

df2.groupBy("Fuel_Type").count.show
+---------+-----+
|Fuel_Type|count|
+---------+-----+
|   Diesel| 3205|
|      CNG|   56|
| Electric|    2|
|      LPG|   10|
|   Petrol| 2746|
+---------+-----+

df2.groupBy("Transmission").count.show
+------------+-----+
|Transmission|count|
+------------+-----+
|   Automatic| 1720|
|      Manual| 4299|
+------------+-----+

df2.groupBy("Owner_Type").count.show
+--------------+-----+
|    Owner_Type|count|
+--------------+-----+
|         First| 4929|
|        Second|  968|
|Fourth & Above|    9|
|         Third|  113|
+--------------+-----+


import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("Location").setOutputCol("LocationCat").setHandleInvalid("skip")
val dfInd2 = new StringIndexer().setInputCol("Fuel_Type").setOutputCol("Fuel_TypeCat").setHandleInvalid("skip")
val dfInd3 = new StringIndexer().setInputCol("Transmission").setOutputCol("TransmissionCat").setHandleInvalid("skip")
val dfInd4 = new StringIndexer().setInputCol("Owner_Type").setOutputCol("Owner_TypeCat").setHandleInvalid("skip")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("Kilometers_Driven","Seats","age","mileage_aux","engine_aux","power_aux", "Fuel_TypeCat","TransmissionCat","Owner_TypeCat","LocationCat"))

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,va))

val df3 = pipeline.fit(df2).transform(df2)

df3.printSchema
root
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: double (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = false)
 |-- Engine: string (nullable = false)
 |-- Power: string (nullable = false)
 |-- Seats: double (nullable = false)
 |-- New_Price: string (nullable = true)
 |-- label: double (nullable = true)
 |-- age: double (nullable = true)
 |-- mileage_aux: double (nullable = true)
 |-- engine_aux: double (nullable = true)
 |-- power_aux: double (nullable = true)
 |-- LocationCat: double (nullable = false)
 |-- Fuel_TypeCat: double (nullable = false)
 |-- TransmissionCat: double (nullable = false)
 |-- Owner_TypeCat: double (nullable = false)
 |-- features: vector (nullable = true)

 
df3.select("label","features").show(10,false)
+-----+--------------------------------------------------+
|label|features                                          |
+-----+--------------------------------------------------+
|1.75 |[72.0,5.0,11.0,26.6,998.0,58.16,2.0,0.0,0.0,0.0]  |
|12.5 |[41.0,5.0,6.0,19.67,1582.0,126.2,0.0,0.0,0.0,4.0] |
|4.5  |[46.0,5.0,10.0,18.2,1199.0,88.7,1.0,0.0,0.0,7.0]  |
|6.0  |[87.0,7.0,9.0,20.77,1248.0,88.76,0.0,0.0,0.0,7.0] |
|17.74|[40.67,5.0,8.0,15.2,1968.0,140.8,0.0,1.0,1.0,3.0] |
|2.35 |[75.0,5.0,9.0,21.1,814.0,55.2,3.0,0.0,0.0,1.0]    |
|3.5  |[86.999,5.0,8.0,23.08,1461.0,63.1,0.0,0.0,0.0,8.0]|
|17.5 |[36.0,8.0,5.0,11.36,2755.0,171.5,0.0,1.0,0.0,0.0] |
|5.2  |[64.43,5.0,8.0,20.54,1598.0,103.6,0.0,0.0,0.0,4.0]|
|1.95 |[65.932,5.0,9.0,22.3,1248.0,74.0,0.0,0.0,1.0,7.0] |
+-----+--------------------------------------------------+
only showing top 10 rows


// ----- building the decision tree model

import org.apache.spark.ml.regression.DecisionTreeRegressor
val dt = new DecisionTreeRegressor
dt.setMaxDepth(10).setMaxBins(32).setFeaturesCol("features")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,va,dt))

val Array(trainingData, testData) = df2.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

val model = pipeline.fit(trainingData)

-- collecting feature importance

import org.apache.spark.ml.regression.DecisionTreeRegressionModel

val dtmodel = model.stages.last.asInstanceOf[DecisionTreeRegressionModel]

val featureImp = va.getInputCols.zip(dtmodel.featureImportances.toArray)

val columns = Array("feature", "Importance")
val featureImpDF = spark.createDataFrame(featureImp).toDF(columns: _*)

featureImpDF.orderBy($"Importance".desc).show()
+-----------------+--------------------+
|          feature|          Importance|
+-----------------+--------------------+
|        power_aux|  0.6317969032132494|
|              age| 0.16717534263276077|
|       engine_aux| 0.04854884601538839|
|      LocationCat| 0.04743465242594365|
|Kilometers_Driven| 0.04334011602536777|
|      mileage_aux| 0.03609146415380448|
|  TransmissionCat|0.010400812588495557|
|            Seats|0.010232154144473201|
|     Fuel_TypeCat|0.004343603316516814|
|    Owner_TypeCat|6.361054840000286E-4|
+-----------------+--------------------+

// -----  metrics extracted from model

val pred = model.transform(trainingData)

import org.apache.spark.ml.evaluation.RegressionEvaluator
val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res8: Double = 1.6346934449884878

bceval.setMetricName("r2").evaluate(pred)
res9: Double = 0.9777442162251505

dtmodel.toDebugString
res10: String =
"DecisionTreeRegressionModel (uid=dtr_6e70eab075d0) of depth 10 with 1181 nodes
  If (feature 5 <= 157.775)
   If (feature 4 <= 1962.0)
    If (feature 2 <= 7.5)
     If (feature 5 <= 103.56)
      If (feature 5 <= 73.985)
       If (feature 5 <= 69.505)
        If (feature 6 in {0.0,1.0,2.0})
         If (feature 4 <= 996.5)
          If (feature 2 <= 5.5)
           If (feature 9 in {5.0,6.0,7.0,9.0,10.0})
            Predict: 2.759333333333333
           Else (feature 9 not in {5.0,6.0,7.0,9.0,10.0})
            Predict: 3.491020408163265
          Else (feature 2 > 5.5)
           If (feature 1 <= 4.5)
            Predict: 1.95625
           Else (feature 1 > 4.5)
            Predict: 2.8235135135135137
         Else (feature 4 > 996.5)
          If (feature 9 in {0....

		   
// -----  metrics on test data

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.RegressionEvaluator
val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res11: Double = 5.078962255317976

bceval.setMetricName("r2").evaluate(pred)
res12: Double = 0.8126673289487728


// ----- find best decision tree model

import org.apache.spark.ml.regression.DecisionTreeRegressor
val dt = new DecisionTreeRegressor
dt.setFeaturesCol("features")

import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,va,dt))

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(dt.maxDepth, Array(7, 10, 20)).
addGrid(dt.maxBins, Array(16, 32, 48)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.DecisionTreeRegressionModel
val dtmodel = bestmodel.stages(5).asInstanceOf[DecisionTreeRegressionModel]

// -----  metrics extracted from model

dtmodel.getMaxDepth
res14: Int = 7

dtmodel.getMaxBins
res15: Int = 16

val pred = bestmodel.transform(trainingData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res16: Double = 3.5127417236677756

bceval.setMetricName("r2").evaluate(pred)
res17: Double = 0.8972308677585834

dtmodel.toDebugString
res18: String =
"DecisionTreeRegressionModel (uid=dtr_8e8932f577af) of depth 7 with 233 nodes
  If (feature 5 <= 157.775)
   If (feature 5 <= 120.35)
    If (feature 2 <= 7.5)
     If (feature 5 <= 100.3)
      If (feature 5 <= 73.92)
       If (feature 6 in {1.0,2.0})
        If (feature 4 <= 996.5)
         Predict: 3.0437500000000006
        Else (feature 4 > 996.5)
         Predict: 3.9672151898734183
       Else (feature 6 not in {1.0,2.0})
        If (feature 7 in {0.0})
         Predict: 4.628846153846155
        Else (feature 7 not in {0.0})
         Predict: 9.879999999999995
      Else (feature 5 > 73.92)
       If (feature 2 <= 5.5)
        If (feature 4 <= 1220.5)
         Predict: 6.158659003831419
        Else (feature 4 > 1220.5)
         Predict: 7.5499206349206345
     ...
	 
// -----  metrics on test data

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res19: Double = 5.260083237075163

bceval.setMetricName("r2").evaluate(pred)
res20: Double = 0.7990681476198392