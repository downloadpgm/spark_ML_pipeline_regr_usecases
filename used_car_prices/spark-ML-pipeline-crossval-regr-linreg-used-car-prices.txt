
val df = spark.read.format("csv").option("inferSchema","true").load("spark/used_cars/used_cars_price_train-data.csv").
         toDF("Row","Name","Location","Year","Kilometers_Driven","Fuel_Type","Transmission","Owner_Type","Mileage","Engine","Power","Seats","New_Price","Price")

df.printSchema
root
 |-- Row: integer (nullable = true)
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = true)
 |-- Engine: string (nullable = true)
 |-- Power: string (nullable = true)
 |-- Seats: double (nullable = true)
 |-- New_Price: string (nullable = true)
 |-- Price: double (nullable = true)

df.describe().show
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+
|summary|               Row|                Name| Location|              Year|Kilometers_Driven|Fuel_Type|Transmission|Owner_Type| Mileage| Engine|   Power|             Seats| New_Price|             Price|
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+
|  count|              6019|                6019|     6019|              6019|             6019|     6019|        6019|      6019|    6017|   5983|    5983|              5977|       824|              6019|
|   mean|            3009.0|                null|     null|2013.3581990363848|58738.38029573019|     null|        null|      null|    null|   null|    null| 5.278735151413753|      null| 9.479468350224273|
| stddev|1737.6799666988932|                null|     null|  3.26974211609139|91268.84320624865|     null|        null|      null|    null|   null|    null|0.8088395547482933|      null|11.187917112455484|
|    min|                 0|Ambassador Classi...|Ahmedabad|              1998|              171|      CNG|   Automatic|     First|0.0 kmpl|1047 CC| 100 bhp|               0.0|      1 Cr|              0.44|
|    max|              6018|Volvo XC90 2007-2...|     Pune|              2019|          6500000|   Petrol|      Manual|     Third|9.9 kmpl| 999 CC|null bhp|              10.0|99.92 Lakh|             160.0|
+-------+------------------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+

df.groupBy("Mileage").count.orderBy('count.desc).show(3)
+---------+-----+
|  Mileage|count|
+---------+-----+
|17.0 kmpl|  172|
|18.9 kmpl|  172|
|18.6 kmpl|  119|
+---------+-----+
only showing top 3 rows

df.groupBy("Engine").count.orderBy('count.desc).show(3)
+-------+-----+
| Engine|count|
+-------+-----+
|1197 CC|  606|
|1248 CC|  512|
|1498 CC|  304|
+-------+-----+
only showing top 3 rows

df.groupBy("Power").count.orderBy('count.desc).show(3)
+--------+-----+
|   Power|count|
+--------+-----+
|  74 bhp|  235|
|98.6 bhp|  131|
|73.9 bhp|  125|
+--------+-----+
only showing top 3 rows

df.groupBy("Seats").count.orderBy('count.desc).show(3)
+-----+-----+
|Seats|count|
+-----+-----+
|  5.0| 5014|
|  7.0|  674|
|  8.0|  134|
+-----+-----+
only showing top 3 rows


val df1 = df.na.fill(Map("Mileage" -> "18.9 kmpl", "Engine" -> "1197 CC", "Power" -> "74 bhp", "Seats" -> 5.0)).
          na.replace("Power", Map("null bhp" -> "74 bhp"))

import org.apache.spark.sql.types._

val df2 = df1.withColumn("age", (lit(2021)-'Year).cast(DoubleType)).
          withColumn("mileage_aux", regexp_extract('Mileage,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumn("engine_aux", regexp_extract('Engine,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumn("power_aux", regexp_extract('Power,"([0-9]+.?[0-9]*)",1).cast(DoubleType)).
          withColumnRenamed("Price", "label").
          drop("Row")

df2.printSchema
root
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = false)
 |-- Engine: string (nullable = false)
 |-- Power: string (nullable = false)
 |-- Seats: double (nullable = false)
 |-- New_Price: string (nullable = true)
 |-- label: double (nullable = true)
 |-- age: double (nullable = true)
 |-- mileage_aux: double (nullable = true)
 |-- engine_aux: double (nullable = true)
 |-- power_aux: double (nullable = true)

df2.describe().show
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+
|summary|                Name| Location|              Year|Kilometers_Driven|Fuel_Type|Transmission|Owner_Type| Mileage| Engine|   Power|             Seats| New_Price|             label|               age|       mileage_aux|        engine_aux|        power_aux|
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+
|  count|                6019|     6019|              6019|             6019|     6019|        6019|      6019|    6019|   6019|    6019|              6019|       824|              6019|              6019|              6019|              6019|             6019|
|   mean|                null|     null|2013.3581990363848|58738.38029573019|     null|        null|      null|    null|   null|    null|5.2767901644791495|      null| 9.479468350224273| 7.641800963615219|18.135215152018656|1618.7388270476824|112.3204718391751|
| stddev|                null|     null|  3.26974211609139|91268.84320624865|     null|        null|      null|    null|   null|    null|0.8063460892297473|      null|11.187917112455484|3.2697421160913938| 4.581548857057788| 600.4458584135865|53.56569974837497|
|    min|Ambassador Classi...|Ahmedabad|              1998|              171|      CNG|   Automatic|     First|0.0 kmpl|1047 CC| 100 bhp|               0.0|      1 Cr|              0.44|               2.0|               0.0|              72.0|             34.2|
|    max|Volvo XC90 2007-2...|     Pune|              2019|          6500000|   Petrol|      Manual|     Third|9.9 kmpl| 999 CC|99.6 bhp|              10.0|99.92 Lakh|             160.0|              23.0|             33.54|            5998.0|            560.0|
+-------+--------------------+---------+------------------+-----------------+---------+------------+----------+--------+-------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+

df2.groupBy("Location").count.show
+----------+-----+
|  Location|count|
+----------+-----+
| Bangalore|  358|
|     Kochi|  651|
|   Chennai|  494|
|    Mumbai|  790|
| Ahmedabad|  224|
|   Kolkata|  535|
|      Pune|  622|
|     Delhi|  554|
|Coimbatore|  636|
| Hyderabad|  742|
|    Jaipur|  413|
+----------+-----+

df2.groupBy("Fuel_Type").count.show
+---------+-----+
|Fuel_Type|count|
+---------+-----+
|   Diesel| 3205|
|      CNG|   56|
| Electric|    2|
|      LPG|   10|
|   Petrol| 2746|
+---------+-----+

df2.groupBy("Transmission").count.show
+------------+-----+
|Transmission|count|
+------------+-----+
|   Automatic| 1720|
|      Manual| 4299|
+------------+-----+

df2.groupBy("Owner_Type").count.show
+--------------+-----+
|    Owner_Type|count|
+--------------+-----+
|         First| 4929|
|        Second|  968|
|Fourth & Above|    9|
|         Third|  113|
+--------------+-----+


import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfInd1 = new StringIndexer().setInputCol("Location").setOutputCol("LocationCat").setHandleInvalid("skip")
val dfInd2 = new StringIndexer().setInputCol("Fuel_Type").setOutputCol("Fuel_TypeCat").setHandleInvalid("skip")
val dfInd3 = new StringIndexer().setInputCol("Transmission").setOutputCol("TransmissionCat").setHandleInvalid("skip")
val dfInd4 = new StringIndexer().setInputCol("Owner_Type").setOutputCol("Owner_TypeCat").setHandleInvalid("skip")

val dfOne1 = new OneHotEncoder().setInputCol("LocationCat").setOutputCol("LocationVect")
val dfOne2 = new OneHotEncoder().setInputCol("Fuel_TypeCat").setOutputCol("Fuel_TypeVect")
val dfOne3 = new OneHotEncoder().setInputCol("TransmissionCat").setOutputCol("TransmissionVect")
val dfOne4 = new OneHotEncoder().setInputCol("Owner_TypeCat").setOutputCol("Owner_TypeVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("Kilometers_Driven","Seats","age","mileage_aux","engine_aux","power_aux", "Fuel_TypeCat","TransmissionCat","Owner_TypeCat","LocationCat"))

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,va))

val df3 = pipeline.fit(df2).transform(df2)

df3.printSchema
root
 |-- Name: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Kilometers_Driven: integer (nullable = true)
 |-- Fuel_Type: string (nullable = true)
 |-- Transmission: string (nullable = true)
 |-- Owner_Type: string (nullable = true)
 |-- Mileage: string (nullable = false)
 |-- Engine: string (nullable = false)
 |-- Power: string (nullable = false)
 |-- Seats: double (nullable = false)
 |-- New_Price: string (nullable = true)
 |-- label: double (nullable = true)
 |-- age: double (nullable = true)
 |-- mileage_aux: double (nullable = true)
 |-- engine_aux: double (nullable = true)
 |-- power_aux: double (nullable = true)
 |-- LocationCat: double (nullable = false)
 |-- Fuel_TypeCat: double (nullable = false)
 |-- TransmissionCat: double (nullable = false)
 |-- Owner_TypeCat: double (nullable = false)
 |-- features: vector (nullable = true)
 
df3.select("label","features").show(10,false)
+-----+----------------------------------------------------+
|label|features                                            |
+-----+----------------------------------------------------+
|1.75 |[72000.0,5.0,11.0,26.6,998.0,58.16,2.0,0.0,0.0,0.0] |
|12.5 |[41000.0,5.0,6.0,19.67,1582.0,126.2,0.0,0.0,0.0,4.0]|
|4.5  |[46000.0,5.0,10.0,18.2,1199.0,88.7,1.0,0.0,0.0,7.0] |
|6.0  |[87000.0,7.0,9.0,20.77,1248.0,88.76,0.0,0.0,0.0,7.0]|
|17.74|[40670.0,5.0,8.0,15.2,1968.0,140.8,0.0,1.0,1.0,3.0] |
|2.35 |[75000.0,5.0,9.0,21.1,814.0,55.2,3.0,0.0,0.0,1.0]   |
|3.5  |[86999.0,5.0,8.0,23.08,1461.0,63.1,0.0,0.0,0.0,8.0] |
|17.5 |[36000.0,8.0,5.0,11.36,2755.0,171.5,0.0,1.0,0.0,0.0]|
|5.2  |[64430.0,5.0,8.0,20.54,1598.0,103.6,0.0,0.0,0.0,4.0]|
|1.95 |[65932.0,5.0,9.0,22.3,1248.0,74.0,0.0,0.0,1.0,7.0]  |
+-----+----------------------------------------------------+
only showing top 10 rows


// calculate pearson correlation to check multicolinearity

import org.apache.spark.ml.stat.Correlation
import org.apache.spark.ml.linalg.Matrix
import org.apache.spark.sql.Row

val corr = Correlation.corr(df3, "features", "pearson").head match {
   case Row(coeff: Matrix) => coeff
}
corr: org.apache.spark.ml.linalg.Matrix =
1.0                   0.08278191382013933    ... (10 total)
0.08278191382013933   1.0                    ...
0.17304756550241057   -0.015204175743670301  ...
-0.06525999805346726  -0.2996488878799701    ...
0.09047629292524847   0.39384940592837275    ...
0.029931845279779875  0.10553944723207119    ...
-0.09888169866324424  -0.29812413609255006   ...
-0.02566271968984989  -0.07483760317989438   ...
0.0853366246443019    0.012210037850503092   ...
0.05329072718709084   0.0012317862165028418  ...


corr.toDense.rowIter.foreach( x => {
  val size = x.size
  for ( i <- Range(0,size)) { 
    val elem = x(i)
    print(f"$elem%.3f\t") 
  }
  println
})
// "Kilometers_Driven","Seats","age","mileage_aux","engine_aux","power_aux", "Fuel_TypeCat","TransmissionCat","Owner_TypeCat","LocationCat"
1.000   0.083   0.173   -0.065  0.090   0.030   -0.099  -0.026  0.085   0.053
0.083   1.000   -0.015  -0.300  0.394   0.106   -0.298  -0.075  0.012   0.001
0.173   -0.015  1.000   -0.322  0.046   -0.040  0.120   -0.097  0.397   0.127
-0.065  -0.300  -0.322  1.000   -0.581  -0.482  -0.076  -0.333  -0.161  0.022
0.090   0.394   0.046   -0.581  1.000   0.860   -0.426  0.499   0.048   -0.026
0.030   0.106   -0.040  -0.482  0.860   1.000   -0.299  0.642   0.014   -0.047
-0.099  -0.298  0.120   -0.076  -0.426  -0.299  1.000   -0.145  0.042   -0.062
-0.026  -0.075  -0.097  -0.333  0.499   0.642   -0.145  1.000   -0.009  -0.081
0.085   0.012   0.397   -0.161  0.048   0.014   0.042   -0.009  1.000   0.132
0.053   0.001   0.127   0.022   -0.026  -0.047  -0.062  -0.081  0.132   1.000

// engine_aux x power_aux = 0.860 can be considered multicolinear
// engine_aux can be removed from analysis

// ----- building the linear regression model

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("Kilometers_Driven","Seats","age","mileage_aux","power_aux", "Fuel_TypeVect","TransmissionVect","Owner_TypeVect","LocationVect"))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
lr.setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd1,dfInd2,dfInd3,dfInd4,dfOne1,dfOne2,dfOne3,dfOne4,va,stdScaler,lr))

val Array(trainingData, testData) = df2.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

// ----- find best linear regression model

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).
addGrid(lr.maxIter, Array(100,200,300)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.LinearRegressionModel
val lrmodel = bestmodel.stages(10).asInstanceOf[LinearRegressionModel]

// -----  metrics extracted from model

lrmodel.getRegParam
res11: Double = 0.1

lrmodel.getMaxIter
res12: Int = 100

lrmodel.getFitIntercept
res4: Boolean = true

lrmodel.getStandardization
res5: Boolean = true

lrmodel.summary.rootMeanSquaredError
res13: Double = 6.0824943988753795

lrmodel.summary.r2
res14: Double = 0.6918702111132775

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.19037933935152807
-0.8377352557827417
-3.1421601025772823
-1.2934703886143841
6.304616617451541
0.31277876991718917
-1.0928469528451656
0.18536581038009703
0.07593518311289596
-1.3824203785754055
0.12942049173108286
-0.10199744261373007
0.11711231862638538
-0.1707494389530703
0.6396942933918182
-0.011933932575626829
0.6274616913788889
0.0485555178960798
-0.3117066800805006
-0.07970533665450887
0.19453622098927636
0.13822726772651883
0.33690146808515065

// -----  metrics on test data

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res16: Double = 6.27667335838381

bceval.setMetricName("r2").evaluate(pred)
res17: Double = 0.7138968684592852
