---- Feature extraction & Data Munging --------------

val hdr = sc.textFile("used_cars/used_cars_price_train-data.hdr").map( x => x.split(",")).take(1)(0)

hdr.take(1)
res0: Array[Array[String]] = Array(Array("", Name, Location, Year, Kilometers_Driven, Fuel_Type, Transmission, Owner_Type, Mileage, Engine, Power, Seats, New_Price, Price))

val rdd = sc.textFile("used_cars/used_cars_price_train-data.csv").map(x => x.split(","))

rdd.take(10).map( x => x.mkString(", ")).foreach(println)
0, Maruti Wagon R LXI CNG, Mumbai, 2010, 72000, CNG, Manual, First, 26.6 km/kg, 998 CC, 58.16 bhp, 5.0, , 1.75
1, Hyundai Creta 1.6 CRDi SX Option, Pune, 2015, 41000, Diesel, Manual, First, 19.67 kmpl, 1582 CC, 126.2 bhp, 5.0, , 12.5
2, Honda Jazz V, Chennai, 2011, 46000, Petrol, Manual, First, 18.2 kmpl, 1199 CC, 88.7 bhp, 5.0, 8.61 Lakh, 4.5
3, Maruti Ertiga VDI, Chennai, 2012, 87000, Diesel, Manual, First, 20.77 kmpl, 1248 CC, 88.76 bhp, 7.0, , 6.0
4, Audi A4 New 2.0 TDI Multitronic, Coimbatore, 2013, 40670, Diesel, Automatic, Second, 15.2 kmpl, 1968 CC, 140.8 bhp, 5.0, , 17.74
5, Hyundai EON LPG Era Plus Option, Hyderabad, 2012, 75000, LPG, Manual, First, 21.1 km/kg, 814 CC, 55.2 bhp, 5.0, , 2.35
6, Nissan Micra Diesel XV, Jaipur, 2013, 86999, Diesel, Manual, First, 23.08 kmpl, 1461 CC, 63.1 bhp, 5.0, , 3.5
7, Toyota Innova Crysta 2.8 GX AT 8S, Mumbai, 2016, 36000, Diesel, Automatic, First, 11.36 kmpl, 2755 CC, 171.5 bhp, 8.0, 21 Lakh, 17.5
8, Volkswagen Vento Diesel Comfortline, Pune, 2013, 64430, Diesel, Manual, First, 20.54 kmpl, 1598 CC, 103.6 bhp, 5.0, , 5.2
9, Tata Indica Vista Quadrajet LS, Chennai, 2012, 65932, Diesel, Manual, Second, 22.3 kmpl, 1248 CC, 74 bhp, 5.0, , 1.95

rdd.count
res1: Long = 6019

val orderingDesc = Ordering.by[(String, Int), Int](_._2)

// Name
rdd.map( x => x(1)).distinct.count
res3: Long = 1876


// Location
rdd.map( x => x(2)).distinct.count
res11: Long = 11

rdd.map( x => (x(2),1) ).reduceByKey(_+_).top(15)(orderingDesc).mkString("\n")
res8: String =
(Mumbai,790)
(Hyderabad,742)
(Kochi,651)
(Coimbatore,636)
(Pune,622)
(Delhi,554)
(Kolkata,535)
(Chennai,494)
(Jaipur,413)
(Bangalore,358)
(Ahmedabad,224)


// Year
rdd.map( x => x(3)).distinct.count
res5: Long = 22

rdd.map( x => (x(3),1) ).reduceByKey(_+_).top(25)(orderingDesc).mkString("\n")
res12: String =
(2014,797)
(2015,744)
(2016,741)
(2013,649)
(2017,587)
(2012,580)
(2011,466)
(2010,342)
(2018,298)
(2009,198)
(2008,174)
(2007,125)
(2019,102)
(2006,78)
(2005,57)
(2004,31)
(2003,17)
(2002,15)
(2001,8)
(2000,4)
(1998,4)
(1999,2)


// Kilometers_Driven
rdd.filter(x => x(4) == "").count
res33: Long = 0


// Fuel_Type
rdd.map( x => x(5)).distinct.count
res6: Long = 5

rdd.map( x => (x(5),1) ).reduceByKey(_+_).top(10)(orderingDesc).mkString("\n")
res14: String =
(Diesel,3205)
(Petrol,2746)
(CNG,56)
(LPG,10)
(Electric,2)


// Transmission
rdd.map( x => x(6)).distinct.count
res7: Long = 2

rdd.map( x => (x(6),1) ).reduceByKey(_+_).top(10)(orderingDesc).mkString("\n")
res15: String =
(Manual,4299)
(Automatic,1720)


// Owner_Type
rdd.map( x => x(7)).distinct.count
res8: Long = 4

rdd.map( x => (x(7),1) ).reduceByKey(_+_).top(10)(orderingDesc).mkString("\n")
res16: String =
(First,4929)
(Second,968)
(Third,113)
(Fourth & Above,9)


// Mileage
rdd.filter(x => x(8) == "").count
res37: Long = 2

rdd.map( x => x(8)).distinct.count
res17: Long = 443

rdd.map( x => (x(8),1) ).reduceByKey(_+_).top(3)(orderingDesc).mkString("\n")
res47: String =
(18.9 kmpl,172)
(17.0 kmpl,172)
(18.6 kmpl,119)


// Engine
rdd.filter(x => x(9) == "").count
res38: Long = 36

rdd.map( x => x(9)).distinct.count
res18: Long = 147

rdd.map( x => (x(9),1) ).reduceByKey(_+_).top(3)(orderingDesc).mkString("\n")
res48: String =
(1197 CC,606)
(1248 CC,512)
(1498 CC,304)


// Power
rdd.filter(x => x(10) == "").count
res39: Long = 36

rdd.map( x => x(10)).distinct.count
res19: Long = 373

rdd.map( x => (x(10),1) ).reduceByKey(_+_).top(3)(orderingDesc).mkString("\n")
res49: String =
(74 bhp,235)
(98.6 bhp,131)
(73.9 bhp,125)


// Seats
rdd.filter(x => x(11) == "").count
res40: Long = 42

rdd.map( x => x(11)).distinct.count
res20: Long = 10

rdd.map( x => (x(11),1) ).reduceByKey(_+_).top(3)(orderingDesc).mkString("\n")
res50: String =
(5.0,5014)
(7.0,674)
(8.0,134)


// New_Price - removed from analysis
rdd.filter(x => x(12) == "").count
res41: Long = 5195


// Price
rdd.filter(x => x(13) == "").count
res42: Long = 0


// for numerical columns, removing embebbed chars and converting to doubles
val rdd1 = rdd.map( x => {
   val model_car = x(1).substring(0,x(1).indexOf(' ',x(1).indexOf(' ',1)+1))
   val age = 2021 - x(3).toDouble
   val km_driven = x(4).toDouble/1000
   //
   val mileage_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(8)).toList
   val mileage = if (mileage_aux.isEmpty) 18.9 else mileage_aux.head.toDouble
   //
   val engine_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(9)).toList
   val engine = if (engine_aux.isEmpty) 1197.0 else engine_aux.head.toDouble
   //
   val power_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(10)).toList
   val power = if (power_aux.isEmpty) 74.0 else power_aux.head.toDouble
   //
   val seats = if (x(11).isEmpty) 5.0 else x(11).toDouble
   val price = x(13).toDouble
   //
   Array(x(0),model_car,x(2),age,km_driven,x(5),x(6),x(7),mileage,engine,power,seats,x(12),price)
 })
 
rdd1.take(10).map( x => x.mkString(", ")).foreach(println)
0, Maruti Wagon, Mumbai, 11.0, 72.0, CNG, Manual, First, 26.6, 1197.0, 58.16, 5.0, , 1.75
1, Hyundai Creta, Pune, 6.0, 41.0, Diesel, Manual, First, 19.67, 1197.0, 126.2, 5.0, , 12.5
2, Honda Jazz, Chennai, 10.0, 46.0, Petrol, Manual, First, 18.2, 1197.0, 88.7, 5.0, 8.61 Lakh, 4.5
3, Maruti Ertiga, Chennai, 9.0, 87.0, Diesel, Manual, First, 20.77, 1197.0, 88.76, 7.0, , 6.0
4, Audi A4, Coimbatore, 8.0, 40.67, Diesel, Automatic, Second, 15.2, 1197.0, 140.8, 5.0, , 17.74
5, Hyundai EON, Hyderabad, 9.0, 75.0, LPG, Manual, First, 21.1, 1197.0, 55.2, 5.0, , 2.35
6, Nissan Micra, Jaipur, 8.0, 86.999, Diesel, Manual, First, 23.08, 1197.0, 63.1, 5.0, , 3.5
7, Toyota Innova, Mumbai, 5.0, 36.0, Diesel, Automatic, First, 11.36, 1197.0, 171.5, 8.0, 21 Lakh, 17.5
8, Volkswagen Vento, Pune, 8.0, 64.43, Diesel, Manual, First, 20.54, 1197.0, 103.6, 5.0, , 5.2
9, Tata Indica, Chennai, 9.0, 65.932, Diesel, Manual, Second, 22.3, 1197.0, 74.0, 5.0, , 1.95


// maker + model will be removed from analysis due to greater number of categories
rdd1.map( x => x(1) ).distinct.count
res11: Long = 216


---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[Any]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  print(hdr(idx) + " : ")
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  println(categories)
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
    if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[Any]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,2,5,6,7)
Location : Map(Bangalore -> 7, Chennai -> 1, Kolkata -> 6, Pune -> 4, Kochi -> 0, Mumbai -> 10, Coimbatore -> 9, Hyderabad -> 3, Ahmedabad -> 8, Delhi -> 2, Jaipur -> 5)
Fuel_Type : Map(Electric -> 3, Petrol -> 2, LPG -> 4, Diesel -> 1, CNG -> 0)
Transmission : Map(Automatic -> 1, Manual -> 0)
Owner_Type : Map(Second -> 1, Third -> 2, First -> 0, Fourth & Above -> 3)

concat.first.size
res29: Int = 21

val rdd2 = rdd1.map( x => Array(x(3),x(4),x(8),x(9),x(10),x(11),x(13))).map( x => x.map( y => y.toString.toDouble ))

rdd2.take(2)
res20: Array[Array[Double]] = Array(Array(11.0, 72.0, 26.6, 1197.0, 58.16, 5.0, 1.75), Array(6.0, 41.0, 19.67, 1197.0, 126.2, 5.0, 12.5))


// merging the numerical columns with 1-of-k vectors produced
val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.first.size
res30: Int = 28

vect.take(10).map( x => x.mkString(", ")).foreach(println)
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 72.0, 26.6, 1197.0, 58.16, 5.0, 1.75
0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 6.0, 41.0, 19.67, 1197.0, 126.2, 5.0, 12.5
0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 10.0, 46.0, 18.2, 1197.0, 88.7, 5.0, 4.5
0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 9.0, 87.0, 20.77, 1197.0, 88.76, 7.0, 6.0
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 8.0, 40.67, 15.2, 1197.0, 140.8, 5.0, 17.74
0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 9.0, 75.0, 21.1, 1197.0, 55.2, 5.0, 2.35
0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 8.0, 86.999, 23.08, 1197.0, 63.1, 5.0, 3.5
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 5.0, 36.0, 11.36, 1197.0, 171.5, 8.0, 17.5
0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 8.0, 64.43, 20.54, 1197.0, 103.6, 5.0, 5.2
0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 9.0, 65.932, 22.3, 1197.0, 74.0, 5.0, 1.95


// Compute summary statistics

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = vect.map( x => Vectors.dense(x) )
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max.toArray.foreach( x => print("%.2f \t".format(x)))
1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00   1.00     1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00   1.00     23.00   6500.00         33.54   1197.00         488.10  10.00   160.00

matrixSummary.min.toArray.foreach( x => print("%.2f \t".format(x)))
0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   0.00     0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   0.00     2.00    0.17    0.00    1197.00         34.20   0.00    0.44

matrixSummary.mean.toArray.foreach( x => print("%.2f \t".format(x)))
0.11    0.08    0.09    0.12    0.10    0.07    0.09    0.06    0.04    0.11   0.13     0.01    0.53    0.46    0.00    0.00    0.29    0.82    0.16    0.02   0.00     7.64    58.74   18.14   1197.00         94.03   5.28    9.48

matrixSummary.variance.toArray.foreach( x => print("%.2f \t".format(x)))
0.10    0.08    0.08    0.11    0.09    0.06    0.08    0.06    0.04    0.09   0.11     0.01    0.25    0.25    0.00    0.00    0.20    0.15    0.13    0.02   0.00     10.69   8330.00         20.99   0.00    1595.92         0.65    125.17


---- Splitting dataset as train/test sets  --------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
  val arr_size = x.size - 1 
  val l = x(arr_size)
  val f = x.slice(0,arr_size)
  LabeledPoint(l,Vectors.dense(f))
})

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache


----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache
testScaled.cache

// ----- building the logistic regression model

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

val model = new LinearRegressionWithSGD

model.setIntercept(true)
model.optimizer.setNumIterations(100).setStepSize(0.1).setRegParam(0.01)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

val metrics = new RegressionMetrics(validPredicts)

metrics.rootMeanSquaredError
res32: Double = 10.449494763990643

metrics.r2
res33: Double = 0.17519217363526962


---- MLlib Linear regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], regSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, setp, reg  -> RMSE, MSE, R2") 
  for(numIter <- iterNums; step <- stepSizes; reg <- regSizes) {
    val model = new LinearRegressionWithSGD
    model.setIntercept(true)
    model.optimizer.setNumIterations(numIter).setStepSize(step).setRegParam(reg)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f, %7.5f -> %.4f, %.4f, %.4f".format(numIter, step, reg, metrics.rootMeanSquaredError, metrics.meanSquaredError, metrics.r2))
  }
}

iterateLRwSGD(Array(10, 20, 40, 100),Array(1, 0.1, 0.01, 0.001),Array(0.1, 0.01), trainScaled, testScaled)
iter, setp, reg  -> RMSE, MSE, R2
10, 1.00000, 0.10000 -> 10.6107, 112.5877, 0.1495
10, 1.00000, 0.01000 -> 10.6107, 112.5877, 0.1495
10, 0.10000, 0.10000 -> 10.8983, 118.7729, 0.1028
10, 0.10000, 0.01000 -> 10.8983, 118.7729, 0.1028
10, 0.01000, 0.10000 -> 13.5421, 183.3895, -0.3853
10, 0.01000, 0.01000 -> 13.5421, 183.3895, -0.3853
10, 0.00100, 0.10000 -> 14.0725, 198.0355, -0.4959
10, 0.00100, 0.01000 -> 14.0725, 198.0355, -0.4959
20, 1.00000, 0.10000 -> 10.5088, 110.4354, 0.1658
20, 1.00000, 0.01000 -> 10.5088, 110.4354, 0.1658
20, 0.10000, 0.10000 -> 10.5223, 110.7198, 0.1637
20, 0.10000, 0.01000 -> 10.5223, 110.7198, 0.1637
20, 0.01000, 0.10000 -> 13.2721, 176.1489, -0.3306
20, 0.01000, 0.01000 -> 13.2721, 176.1489, -0.3306
20, 0.00100, 0.10000 -> 14.0405, 197.1346, -0.4891
20, 0.00100, 0.01000 -> 14.0405, 197.1346, -0.4891
40, 1.00000, 0.10000 -> 10.4759, 109.7452, 0.1710
40, 1.00000, 0.01000 -> 10.4759, 109.7452, 0.1710
40, 0.10000, 0.10000 -> 10.3818, 107.7811, 0.1858 *
40, 0.10000, 0.01000 -> 10.3818, 107.7811, 0.1858
40, 0.01000, 0.10000 -> 12.9225, 166.9907, -0.2614
40, 0.01000, 0.01000 -> 12.9225, 166.9907, -0.2614
40, 0.00100, 0.10000 -> 13.9952, 195.8650, -0.4795
40, 0.00100, 0.01000 -> 13.9952, 195.8650, -0.4795
100, 1.00000, 0.10000 -> 10.4759, 109.7452, 0.1710
100, 1.00000, 0.01000 -> 10.4759, 109.7452, 0.1710
100, 0.10000, 0.10000 -> 10.4495, 109.1919, 0.1752
100, 0.10000, 0.01000 -> 10.4495, 109.1919, 0.1752
100, 0.01000, 0.10000 -> 12.3353, 152.1590, -0.1494
100, 0.01000, 0.01000 -> 12.3353, 152.1590, -0.1494
100, 0.00100, 0.10000 -> 13.9064, 193.3873, -0.4608
100, 0.00100, 0.01000 -> 13.9064, 193.3873, -0.4608

val model = new LinearRegressionWithSGD

model.setIntercept(true)
model.optimizer.setNumIterations(40).setStepSize(0.1).setRegParam(0.1)

val lr = model.run(trainScaled)

val validPredicts = testScaled.map(x => (lr.predict(x.features),x.label))

val metrics = new RegressionMetrics(validPredicts)

metrics.rootMeanSquaredError
res38: Double = 10.381767082865487

metrics.r2
res39: Double = 0.18584939464265415

lr.weights.toArray.map( x => x.abs ).zipWithIndex.sortBy(-_._1).foreach(println)
(3.2845311291102655,16)
(2.053892626346267,25)
(1.9563840425177803,23)
(1.6691157512068875,21)
(1.4473547340212058,12)
(1.4252062005858737,13)
(1.0839605666653516,22)
(0.8476983065101621,9)
(0.5981076167727655,6)
(0.3792329558925968,7)
(0.2927672733963169,5)
(0.2392487934666181,4)
(0.23037994931359293,17)
(0.18175156020930916,18)
(0.1710115409704622,0)
(0.15354437555222777,1)
(0.13995908267333382,19)
(0.10073403105489538,10)
(0.09656760972525555,11)
(0.08074032746553672,8)
(0.07464533865246425,26)
(0.0674098475833028,20)
(0.06576945868979123,15)
(0.009865752825069164,3)
(0.008042731815551351,2)
(0.0,14)
(0.0,24)