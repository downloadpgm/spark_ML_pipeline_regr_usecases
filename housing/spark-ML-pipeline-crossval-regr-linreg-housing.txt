
import org.apache.spark.sql.functions._

val df = spark.read.format("csv").option("inferSchema","true").option("ignoreLeadingWhiteSpace","true").load("staging/housing.data").toDF("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")

val df1 = df.where('CRIM < 27.0).
             withColumn("CRIM_log", log('CRIM)).
             withColumn("DIS_log", log('DIS))	

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

// ----- find best linear regression model

val listcols = df1.columns.diff(Array("CRIM","DIS","MEDV"))

import org.apache.spark.ml.feature.RFormula
val rf = new RFormula().setFormula("MEDV ~ " + listcols.mkString(" + "))

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
lr.setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(rf,stdScaler,lr))

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.maxIter, Array(10, 20, 40, 100)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.LinearRegressionModel
val lrmodel = bestmodel.stages(2).asInstanceOf[LinearRegressionModel]

// -----  metrics extracted from model

lrmodel.getRegParam
res1: Double = 0.1

lrmodel.getMaxIter
res2: Int = 10

lrmodel.getFitIntercept
res4: Boolean = true

lrmodel.getStandardization
res5: Boolean = true

lrmodel.summary.rootMeanSquaredError
res25: Double = 4.6036927032226505

lrmodel.summary.r2
res26: Double = 0.7483954793305998

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.7745049833943773   // ZN
-0.7004639873397853  // INDUS
0.399412002029792    // CHAS
-2.7245464454615     // NOX
2.7190362000423463   // RM
-0.38915049559187165 // AGE
1.9174996085770104   // RAD
-2.157970584109642   // TAX
-1.7695157032836102  // PTRATIO
0.9367057239820256   // B
-4.059996056719011   // LSTAT
0.777060686619494    // CRIM_log
-4.2791101110080865  // DIS_log

// -----  metrics on test data

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res28: Double = 4.381952181180771

bceval.setMetricName("r2").evaluate(pred)
res29: Double = 0.756633136500374


// ----- building reduced linear regression model

import org.apache.spark.ml.feature.RFormula
val rf = new RFormula().setFormula("MEDV ~ NOX + RM + RAD + TAX + PTRATIO + LSTAT + DIS_log")

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
lr.setFitIntercept(true).setRegParam(0.1).setMaxIter(10).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(rf,stdScaler,lr))

val pipelinemodel = pipeline.fit(df1)

val pred = pipelinemodel.transform(df1)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res32: Double = 4.679014694183168

bceval.setMetricName("r2").evaluate(pred)
res33: Double = 0.7350615478560851