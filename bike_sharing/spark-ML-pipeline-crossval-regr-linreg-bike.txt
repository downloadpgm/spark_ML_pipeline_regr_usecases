
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("spark/bike/hour.csv")

df.printSchema
root
 |-- instant: integer (nullable = true)
 |-- dteday: timestamp (nullable = true)
 |-- season: integer (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: integer (nullable = true)
 |-- hr: integer (nullable = true)
 |-- holiday: integer (nullable = true)
 |-- weekday: integer (nullable = true)
 |-- workingday: integer (nullable = true)
 |-- weathersit: integer (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- casual: integer (nullable = true)
 |-- registered: integer (nullable = true)
 |-- cnt: integer (nullable = true)
 
import org.apache.spark.sql.types._

val df1 = df.drop("instant","casual","registered").
withColumn("season",'season.cast(DoubleType)).
withColumn("mnth",'mnth.cast(DoubleType)).
withColumn("hr",'hr.cast(DoubleType)).
withColumn("holiday",'holiday.cast(DoubleType)).
withColumn("weekday",'weekday.cast(DoubleType)).
withColumn("workingday",'workingday.cast(DoubleType)).
withColumn("weathersit",'weathersit.cast(DoubleType)).
withColumn("tod", when(col("hr") >= 6 && col("hr") < 12, "morning" ).
                   when(col("hr") >=12 and col("hr") < 17, "afternoon").
                   when(col("hr") >=17 and col("hr") < 23, "evening").
                   otherwise("night"))

df1.printSchema
root
 |-- dteday: timestamp (nullable = true)
 |-- season: double (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: double (nullable = true)
 |-- hr: double (nullable = true)
 |-- holiday: double (nullable = true)
 |-- weekday: double (nullable = true)
 |-- workingday: double (nullable = true)
 |-- weathersit: double (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- cnt: integer (nullable = true)
 |-- tod: string (nullable = false)

df1.show(10)
+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+-------+
|             dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|cnt|    tod|
+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+-------+
|2011-01-01 00:00:00|   1.0|  0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.81|      0.0| 16|  night|
|2011-01-01 00:00:00|   1.0|  0| 1.0|1.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0| 40|  night|
|2011-01-01 00:00:00|   1.0|  0| 1.0|2.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0| 32|  night|
|2011-01-01 00:00:00|   1.0|  0| 1.0|3.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0| 13|  night|
|2011-01-01 00:00:00|   1.0|  0| 1.0|4.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|  1|  night|
|2011-01-01 00:00:00|   1.0|  0| 1.0|5.0|    0.0|    6.0|       0.0|       2.0|0.24|0.2576|0.75|   0.0896|  1|  night|
|2011-01-01 00:00:00|   1.0|  0| 1.0|6.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|  2|morning|
|2011-01-01 00:00:00|   1.0|  0| 1.0|7.0|    0.0|    6.0|       0.0|       1.0| 0.2|0.2576|0.86|      0.0|  3|morning|
|2011-01-01 00:00:00|   1.0|  0| 1.0|8.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|  8|morning|
|2011-01-01 00:00:00|   1.0|  0| 1.0|9.0|    0.0|    6.0|       0.0|       1.0|0.32|0.3485|0.76|      0.0| 14|morning|
+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+-------+
only showing top 10 rows

df1.describe().show
+-------+------------------+------------------+------------------+------------------+--------------------+-----------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+---------+
|summary|            season|                yr|              mnth|                hr|             holiday|          weekday|        workingday|        weathersit|               temp|             atemp|                hum|          windspeed|               cnt|      tod|
+-------+------------------+------------------+------------------+------------------+--------------------+-----------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+---------+
|  count|             17379|             17379|             17379|             17379|               17379|            17379|             17379|             17379|              17379|             17379|              17379|              17379|             17379|    17379|
|   mean|2.5016399102364923|0.5025605615973301| 6.537775476149376|11.546751826917545|0.028770355026181024|3.003682605443351|0.6827205247712756| 1.425283387997008| 0.4969871684216586|0.4757751021347581| 0.6272288394038822| 0.1900976063064631|189.46308763450142|     null|
| stddev|  1.10691813944808|0.5000078290910193|3.4387757137501724|6.9144050952644776|  0.1671652763843717|2.005771456110986|0.4654306335238818|0.6393568777542525|0.19255612124972202|0.1718502156353594|0.19292983406291458|0.12234022857279034| 181.3875990918646|     null|
|    min|               1.0|                 0|               1.0|               0.0|                 0.0|              0.0|               0.0|               1.0|               0.02|               0.0|                0.0|                0.0|                 1|afternoon|
|    max|               4.0|                 1|              12.0|              23.0|                 1.0|              6.0|               1.0|               4.0|                1.0|               1.0|                1.0|             0.8507|               977|    night|
+-------+------------------+------------------+------------------+------------------+--------------------+-----------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+---------+

val df2 = df1.groupBy('season,'yr,'mnth,'tod,'holiday,'weekday,'workingday,'weathersit).agg(avg('temp).as("avg_temp"),avg('atemp).as("avg_atemp"),avg('hum).as("avg_hum"),avg('windspeed).as("avg_windspeed"),sum('cnt).as("total_cnt"))

df2.printSchema
root
 |-- season: double (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: double (nullable = true)
 |-- tod: string (nullable = false)
 |-- holiday: double (nullable = true)
 |-- weekday: double (nullable = true)
 |-- workingday: double (nullable = true)
 |-- weathersit: double (nullable = true)
 |-- avg_temp: double (nullable = true)
 |-- avg_atemp: double (nullable = true)
 |-- avg_hum: double (nullable = true)
 |-- avg_windspeed: double (nullable = true)
 |-- total_cnt: long (nullable = true)
 
df2.show(10)
+------+---+----+---------+-------+-------+----------+----------+-------------------+-------------------+------------------+-------------------+---------+
|season| yr|mnth|      tod|holiday|weekday|workingday|weathersit|           avg_temp|          avg_atemp|           avg_hum|      avg_windspeed|total_cnt|
+------+---+----+---------+-------+-------+----------+----------+-------------------+-------------------+------------------+-------------------+---------+
|   2.0|  0| 3.0|  evening|    0.0|    0.0|       0.0|       1.0| 0.2866666666666667| 0.2777833333333333|             0.355|0.19403333333333336|      522|
|   2.0|  0| 4.0|  morning|    0.0|    2.0|       1.0|       2.0|  0.508888888888889|0.49158888888888885|0.7133333333333333| 0.1874111111111111|     1170|
|   2.0|  0| 4.0|  morning|    0.0|    4.0|       1.0|       1.0|0.41600000000000004|0.41209999999999997|0.5890000000000001|            0.18507|     1583|
|   3.0|  0| 7.0|afternoon|    0.0|    4.0|       1.0|       1.0| 0.8389473684210527| 0.7878684210526318|0.4378947368421053| 0.2325157894736842|     3416|
|   4.0|  0| 9.0|  evening|    0.0|    5.0|       1.0|       2.0|                0.6|                0.5|               1.0|                0.0|       99|
|   1.0|  1| 1.0|    night|    0.0|    5.0|       1.0|       2.0|              0.358|            0.35605|0.8019999999999999|0.29552000000000006|      218|
|   1.0|  1| 1.0|    night|    0.0|    1.0|       1.0|       2.0|0.21399999999999997|            0.24394|              0.78|            0.11494|      166|
|   2.0|  1| 3.0|  morning|    0.0|    6.0|       0.0|       2.0|0.44909090909090915|0.43800909090909085|0.8572727272727274| 0.1695909090909091|     1867|
|   3.0|  1| 7.0|    night|    0.0|    5.0|       1.0|       3.0|               0.66|             0.5909|              0.89|             0.0896|       34|
|   4.0|  1|10.0|afternoon|    0.0|    2.0|       1.0|       3.0|0.43428571428571416|0.40044285714285716|              0.85|0.23238571428571433|     1210|
+------+---+----+---------+-------+-------+----------+----------+-------------------+-------------------+------------------+-------------------+---------+
only showing top 10 rows


val df3 = df2.withColumn("label", 'total_cnt.cast(DoubleType))

import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

val dfInd3 = new StringIndexer().setInputCol("tod").setOutputCol("todCat")

val dfOne1 = new OneHotEncoder().setInputCol("season").setOutputCol("seasonVect")
val dfOne2 = new OneHotEncoder().setInputCol("mnth").setOutputCol("mnthVect")
val dfOne3 = new OneHotEncoder().setInputCol("todCat").setOutputCol("todVect")
val dfOne4 = new OneHotEncoder().setInputCol("weekday").setOutputCol("weekdayVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("season","mnth","todCat","holiday","weekday","workingday","weathersit","avg_temp","avg_atemp","avg_hum","avg_windspeed"))

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd3,va))

val df4 = pipeline.fit(df3).transform(df3)

df4.printSchema
root
 |-- season: double (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: double (nullable = true)
 |-- tod: string (nullable = false)
 |-- holiday: double (nullable = true)
 |-- weekday: double (nullable = true)
 |-- workingday: double (nullable = true)
 |-- weathersit: double (nullable = true)
 |-- avg_temp: double (nullable = true)
 |-- avg_atemp: double (nullable = true)
 |-- avg_hum: double (nullable = true)
 |-- avg_windspeed: double (nullable = true)
 |-- total_cnt: long (nullable = true)
 |-- label: double (nullable = true)
 |-- todCat: double (nullable = false)
 |-- features: vector (nullable = true)
 
df4.select("label","features").show(10,false)
+------------------+-----------------------------------------------------------------------------------------------------------+
|label             |features                                                                                                   |
+------------------+-----------------------------------------------------------------------------------------------------------+
|9.027905996569885 |[2.0,3.0,2.0,0.0,0.0,0.0,1.0,0.2866666666666667,0.2777833333333333,0.355,0.19403333333333336]              |
|10.192292814470768|[2.0,4.0,1.0,0.0,2.0,1.0,2.0,0.508888888888889,0.49158888888888885,0.7133333333333333,0.1874111111111111]  |
|10.628445540137182|[2.0,4.0,1.0,0.0,4.0,1.0,1.0,0.41600000000000004,0.41209999999999997,0.5890000000000001,0.18507]           |
|11.73809225962049 |[3.0,7.0,3.0,0.0,4.0,1.0,1.0,0.8389473684210527,0.7878684210526318,0.4378947368421053,0.2325157894736842]  |
|6.6293566200796095|[4.0,9.0,2.0,0.0,5.0,1.0,2.0,0.6,0.5,1.0,0.0]                                                              |
|7.768184324776926 |[1.0,1.0,0.0,0.0,5.0,1.0,2.0,0.358,0.35605,0.8019999999999999,0.29552000000000006]                         |
|7.375039431346925 |[1.0,1.0,0.0,0.0,1.0,1.0,2.0,0.21399999999999997,0.24394,0.78,0.11494]                                     |
|10.866506212226202|[2.0,3.0,1.0,0.0,6.0,0.0,2.0,0.44909090909090915,0.43800909090909085,0.8572727272727274,0.1695909090909091]|
|5.08746284125034  |[3.0,7.0,0.0,0.0,5.0,1.0,3.0,0.66,0.5909,0.89,0.0896]                                                      |
|10.240791332161956|[4.0,10.0,3.0,0.0,2.0,1.0,3.0,0.43428571428571416,0.40044285714285716,0.85,0.23238571428571433]            |
+------------------+-----------------------------------------------------------------------------------------------------------+
only showing top 10 rows


// calculate pearson correlation to check multicolinearity

import org.apache.spark.ml.stat.Correlation
import org.apache.spark.ml.linalg.Matrix
import org.apache.spark.sql.Row

val corr = Correlation.corr(df4, "features", "pearson").head match {
   case Row(coeff: Matrix) => coeff
}
corr: org.apache.spark.ml.linalg.Matrix =
1.0                    0.7412113233649982     ... (11 total)
0.7412113233649982     1.0                    ...
0.010065463078705898   0.008055104632046436   ...
-0.006736973362864755  0.020586950584640665   ...
-0.016786900441882004  0.006259741794439168   ...
0.022601055986971135   0.015017439400125314   ...
-0.030163581081520233  -0.021276673114402803  ...
0.3751042462264109     0.19231582395845767    ...
0.38030761654795125    0.19609196262115736    ...
0.1269383359380271     0.14509855098689942    ...
-0.17523129966973117   -0.1400201523978377    ...


corr.toDense.rowIter.foreach( x => {
  val size = x.size
  for ( i <- Range(0,size)) { 
    val elem = x(i)
    print(f"$elem%.3f\t") 
  }
  println
})
// "season","mnth","todCat","holiday","weekday","workingday","weathersit","avg_temp","avg_atemp","avg_hum","avg_windspeed"
1.000   0.741   0.010   -0.007  -0.017  0.023   -0.030  0.375   0.380   0.127  -0.175
0.741   1.000   0.008   0.021   0.006   0.015   -0.021  0.192   0.196   0.145  -0.140
0.010   0.008   1.000   -0.007  0.001   -0.000  -0.025  0.186   0.187   -0.402 0.251
-0.007  0.021   -0.007  1.000   -0.167  -0.362  -0.070  -0.014  -0.020  -0.027 -0.007
-0.017  0.006   0.001   -0.167  1.000   0.065   -0.001  -0.020  -0.027  -0.034 0.025
0.023   0.015   -0.000  -0.362  0.065   1.000   0.050   0.054   0.057   0.014  -0.003
-0.030  -0.021  -0.025  -0.070  -0.001  0.050   1.000   -0.064  -0.070  0.534  0.070
0.375   0.192   0.186   -0.014  -0.020  0.054   -0.064  1.000   0.993   -0.041 -0.061
0.380   0.196   0.187   -0.020  -0.027  0.057   -0.070  0.993   1.000   -0.040 -0.086
0.127   0.145   -0.402  -0.027  -0.034  0.014   0.534   -0.041  -0.040  1.000  -0.258
-0.175  -0.140  0.251   -0.007  0.025   -0.003  0.070   -0.061  -0.086  -0.258 1.000

// avg_temp x avg_atemp = 0.993 can be considered multicolinear
// avg_atemp removed from analysis

// ----- building the linear regression model

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("seasonVect","mnthVect","todVect","holiday","weekdayVect","workingday","weathersit","avg_temp","avg_hum","avg_windspeed"))

import org.apache.spark.ml.feature.StandardScaler

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
lr.setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd3,dfOne1,dfOne2,dfOne3,dfOne4,va,stdScaler,lr))

val Array(trainingData, testData) = df3.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

// ----- find best linear regression model

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.maxIter, Array(10,20,50,100)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.LinearRegressionModel
val lrmodel = bestmodel.stages(7).asInstanceOf[LinearRegressionModel]

// -----  metrics extracted from model

lrmodel.getRegParam
res24: Double = 1.0

lrmodel.getMaxIter
res2: Int = 100

lrmodel.getFitIntercept
res4: Boolean = true

lrmodel.getStandardization
res5: Boolean = true

lrmodel.summary.rootMeanSquaredError
res26: Double = 1299.9018304356196

lrmodel.summary.r2
res27: Double = 0.4862328996882922

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.0
57.41485166317221
224.835356136308
99.84127903336822
0.0
98.2400077689785
58.44571531174944
-170.72166673433733
-3.750185331241917
24.310116687369465
-333.87569997649393
-61.98824382358321
-11.233138898866265
-119.81409491623478
178.53980872032062
161.05457550958764
-455.85014365729984
-18.730873487900766
-174.61406540620135
-297.2019900951199
-21.65327963325743
-109.06982736099512
-6.833522027627914
-34.79705713947928
-30.67161856756583
-6.046585921698404
7.211691101161092
-740.4572120577287
645.4502759762097
-276.19834884868857
-18.347769658459192

// -----  metrics on test data

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res29: Double = 1237.5036737173723

bceval.setMetricName("r2").evaluate(pred)
res30: Double = 0.4441550337975986


// transforming cnt to log2

import org.apache.spark.sql.functions._
val df3 = df2.withColumn("label", log2('total_cnt.cast(DoubleType)))

// ----- building the linear regression model

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("seasonVect","mnthVect","todVect","holiday","weekdayVect","workingday","weathersit","avg_temp","avg_hum","avg_windspeed"))

import org.apache.spark.ml.feature.StandardScaler

import org.apache.spark.ml.feature.StandardScaler
val stdScaler = new StandardScaler().
setWithStd(true).
setWithMean(true).
setInputCol("features").
setOutputCol("scaledFeatures")

import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression
lr.setFitIntercept(true).setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfInd3,dfOne1,dfOne2,dfOne3,dfOne4,va,stdScaler,lr))

val Array(trainingData, testData) = df3.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

// ----- find best linear regression model

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.maxIter, Array(10,20,50,100)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.LinearRegressionModel
val lrmodel = bestmodel.stages(7).asInstanceOf[LinearRegressionModel]

// -----  metrics extracted from model

lrmodel.getRegParam
res24: Double = 1.0

lrmodel.getMaxIter
res2: Int = 100

lrmodel.getFitIntercept
res4: Boolean = true

lrmodel.getStandardization
res5: Boolean = true

lrmodel.summary.rootMeanSquaredError
res26: Double = 1299.9018304356196

lrmodel.summary.r2
res27: Double = 0.4862328996882922

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.0
57.41485166317221
224.835356136308
99.84127903336822
0.0
98.2400077689785
58.44571531174944
-170.72166673433733
-3.750185331241917
24.310116687369465
-333.87569997649393
-61.98824382358321
-11.233138898866265
-119.81409491623478
178.53980872032062
161.05457550958764
-455.85014365729984
-18.730873487900766
-174.61406540620135
-297.2019900951199
-21.65327963325743
-109.06982736099512
-6.833522027627914
-34.79705713947928
-30.67161856756583
-6.046585921698404
7.211691101161092
-740.4572120577287
645.4502759762097
-276.19834884868857
-18.347769658459192

// -----  metrics on test data

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res29: Double = 1237.5036737173723

bceval.setMetricName("r2").evaluate(pred)
res30: Double = 0.4441550337975986
