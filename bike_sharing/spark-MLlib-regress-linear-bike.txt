
val rdd = sc.textFile("spark/bike/hour.csv")

rdd.take(10)
res3: Array[String] = Array(instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt, 1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16, 2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40, 3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32, 4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13, 5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1, 6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1, 7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2, 8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3, 9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8)

rdd.take(1)
res0: Array[String] = Array(instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt)

val rdd_nohdr = rdd.filter( x => ! x.contains("instant"))

val rdd1 = rdd_nohdr.map( x => x.split(",")).map( x => x.map( y => try { y.toDouble } catch { case e: Throwable => y } ))

rdd1.take(2)
res2: Array[Array[Any]] = Array(Array(1.0, 2011-01-01, 1.0, 0.0, 1.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.24, 0.2879, 0.81, 0.0, 3.0, 13.0, 16.0), Array(2.0, 2011-01-01, 1.0, 0.0, 1.0, 1.0, 0.0, 6.0, 0.0, 1.0, 0.22, 0.2727, 0.8, 0.0, 8.0, 32.0, 40.0))

val rdd2 = rdd1.map( x => {
   val tod = x(5) match {
      case 6|7|8|9|10|11 => "morning"
      case 12|13|14|15|16 => "afternoon"
      case 17|18|19|20|21|22 => "evening"
      case _ => "night"
      }
   // season,yr,mnth,tod,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,cnt
   Array(x(2),x(3),x(4),tod,x(6),x(7),x(8),x(9),x(10),x(11),x(12),x(13),x(16)) 
})
	  
rdd2.take(5)
res0: Array[Array[Any]] = Array(Array(1.0, 0.0, 1.0, night, 0.0, 6.0, 0.0, 1.0, 0.24, 0.2879, 0.81, 0.0, 16.0), Array(1.0, 0.0, 1.0, night, 0.0, 6.0, 0.0, 1.0, 0.22, 0.2727, 0.8, 0.0, 40.0), Array(1.0, 0.0, 1.0, night, 0.0, 6.0, 0.0, 1.0, 0.22, 0.2727, 0.8, 0.0, 32.0), Array(1.0, 0.0, 1.0, night, 0.0, 6.0, 0.0, 1.0, 0.24, 0.2879, 0.75, 0.0, 13.0), Array(1.0, 0.0, 1.0, night, 0.0, 6.0, 0.0, 1.0, 0.24, 0.2879, 0.75, 0.0, 1.0))

val rdd3 = rdd2.keyBy(x => (x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7)))
rdd3: org.apache.spark.rdd.RDD[((Any, Any, Any, Any, Any, Any, Any, Any), Array[Any])] = MapPartitionsRDD[10] at keyBy at <console>:25

val rdd4 = rdd3.mapValues( x => Array(x(8).toString.toDouble,x(9).toString.toDouble,x(10).toString.toDouble,x(11).toString.toDouble,x(12).toString.toDouble,1))
rdd4: org.apache.spark.rdd.RDD[((Any, Any, Any, Any, Any, Any, Any, Any), Array[Double])] = MapPartitionsRDD[56] at mapValues at <console>:25

rdd4.take(5)
res1: Array[((Any, Any, Any, Any, Any, Any, Any, Any), Array[Double])] = Array(((1.0,0.0,1.0,night,0.0,6.0,0.0,1.0),Array(0.24, 0.2879, 0.81, 0.0, 16.0, 1.0)), ((1.0,0.0,1.0,night,0.0,6.0,0.0,1.0),Array(0.22, 0.2727, 0.8, 0.0, 40.0, 1.0)), ((1.0,0.0,1.0,night,0.0,6.0,0.0,1.0),Array(0.22, 0.2727, 0.8, 0.0, 32.0, 1.0)), ((1.0,0.0,1.0,night,0.0,6.0,0.0,1.0),Array(0.24, 0.2879, 0.75, 0.0, 13.0, 1.0)), ((1.0,0.0,1.0,night,0.0,6.0,0.0,1.0),Array(0.24, 0.2879, 0.75, 0.0, 1.0, 1.0)))

def reduceSum(a1: Array[Double], a2: Array[Double]): Array[Double] = {
  Array(a1(0)+a2(0),a1(1)+a2(1),a1(2)+a2(2),a1(3)+a2(3),a1(4)+a2(4),a1(5)+a2(5))
}

val summed = rdd4.reduceByKey(reduceSum)

summed.take(5)
res2: Array[((Any, Any, Any, Any, Any, Any, Any, Any), Array[Double])] = Array(((4.0,0.0,10.0,night,0.0,5.0,1.0,2.0),Array(0.8600000000000001, 0.8182, 1.5499999999999998, 0.4179, 42.0, 2.0)), ((4.0,1.0,12.0,night,0.0,1.0,1.0,2.0),Array(4.18, 4.333, 10.059999999999999, 0.6867, 148.0, 11.0)), ((3.0,0.0,7.0,evening,0.0,5.0,1.0,1.0),Array(19.380000000000003, 17.7881, 11.860000000000001, 4.5824, 7609.0, 25.0)), ((1.0,1.0,3.0,evening,0.0,1.0,1.0,2.0),Array(4.0, 3.8788, 3.64, 1.6717, 2574.0, 7.0)), ((3.0,1.0,7.0,afternoon,0.0,6.0,0.0,1.0),Array(8.82, 7.954500000000001, 3.7800000000000002, 2.0598, 4597.0, 10.0)))

val avged = summed.mapValues( x => Array(x(0)/x(5), x(1)/x(5), x(2)/x(5), x(3)/x(5), x(4), x(5)))
avged: org.apache.spark.rdd.RDD[((Any, Any, Any, Any, Any, Any, Any, Any), Array[Double])] = MapPartitionsRDD[60] at mapValues at <console>:25

avged.take(5)
res3: Array[((Any, Any, Any, Any, Any, Any, Any, Any), Array[Double])] = Array(((4.0,0.0,10.0,night,0.0,5.0,1.0,2.0),Array(0.43000000000000005, 0.4091, 0.7749999999999999, 0.20895, 42.0, 2.0)), ((4.0,1.0,12.0,night,0.0,1.0,1.0,2.0),Array(0.37999999999999995, 0.39390909090909093, 0.9145454545454544, 0.06242727272727273, 148.0, 11.0)), ((3.0,0.0,7.0,evening,0.0,5.0,1.0,1.0),Array(0.7752000000000001, 0.711524, 0.47440000000000004, 0.183296, 7609.0, 25.0)), ((1.0,1.0,3.0,evening,0.0,1.0,1.0,2.0),Array(0.5714285714285714, 0.5541142857142857, 0.52, 0.2388142857142857, 2574.0, 7.0)), ((3.0,1.0,7.0,afternoon,0.0,6.0,0.0,1.0),Array(0.882, 0.7954500000000001, 0.378, 0.20598, 4597.0, 10.0)))

val flatted = avged.map{ case (k,v) => Array(k._1,k._2,k._3,k._4,k._5,k._6,k._7,k._8,v(0),v(1),v(2),v(3),v(4)) }
flatted: org.apache.spark.rdd.RDD[Array[Any]] = MapPartitionsRDD[63] at map at <console>:25

flatted.take(5)
res4: Array[Array[Any]] = Array(Array(4.0, 0.0, 10.0, night, 0.0, 5.0, 1.0, 2.0, 0.43000000000000005, 0.4091, 0.7749999999999999, 0.20895, 42.0), Array(4.0, 1.0, 12.0, night, 0.0, 1.0, 1.0, 2.0, 0.37999999999999995, 0.39390909090909093, 0.9145454545454544, 0.06242727272727273, 148.0), Array(3.0, 0.0, 7.0, evening, 0.0, 5.0, 1.0, 1.0, 0.7752000000000001, 0.711524, 0.47440000000000004, 0.183296, 7609.0), Array(1.0, 1.0, 3.0, evening, 0.0, 1.0, 1.0, 2.0, 0.5714285714285714, 0.5541142857142857, 0.52, 0.2388142857142857, 2574.0), Array(3.0, 1.0, 7.0, afternoon, 0.0, 6.0, 0.0, 1.0, 0.882, 0.7954500000000001, 0.378, 0.20598, 4597.0))


// cnt distribution on histogram

val rdd_cnt = flatted.map( x => x(12).toString.toDouble)
rdd_cnt: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[17] at map at <console>:25

rdd_cnt.histogram(20)
res9: (Array[Double], Array[Long]) = (Array(1.0, 765.15, 1529.3, 2293.45, 3057.6, 3821.75, 4585.9, 5350.05, 6114.2, 6878.35, 7642.5, 8406.65, 9170.8, 9934.95, 10699.1, 11463.25, 12227.4, 12991.55, 13755.7, 14519.85, 15284.0),Array(1084, 394, 222, 136, 106, 65, 42, 40, 24, 9, 15, 14, 8, 10, 1, 1, 0, 0, 2, 1))

import scala.math._

val rdd_logcnt = rdd_cnt.map( x => log(x))
rdd_logcnt: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[62] at map at <console>:28

rdd_logcnt.histogram(20)
res17: (Array[Double], Array[Long]) = (Array(0.0, 0.48172809042833953, 0.9634561808566791, 1.4451842712850185, 1.9269123617133581, 2.4086404521416975, 2.890368542570037, 3.372096632998377, 3.8538247234267162, 4.335552813855055, 4.817280904283395, 5.299008994711735, 5.780737085140074, 6.262465175568414, 6.744193265996754, 7.225921356425093, 7.7076494468534325, 8.189377537281771, 8.67110562771011, 9.15283371813845, 9.63456180856679),Array(2, 2, 4, 6, 21, 17, 29, 54, 85, 110, 154, 216, 209, 237, 273, 264, 233, 162, 75, 21))


---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[Any]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
    val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
    if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[Any]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(flatted,0,2,3,5,7)

concat.take(5)
res10: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0), Array(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0....

concat.first.size
res1: Int = 31

val rdd5 = flatted.map( x => Array(x(1),x(4),x(6),x(8),x(9),x(10),x(11),x(12)) )

rdd5.take(5)
res12: Array[Array[Any]] = Array(Array(0.0, 0.0, 1.0, 0.43000000000000005, 0.4091, 0.7749999999999999, 0.20895, 42.0), Array(1.0, 0.0, 1.0, 0.37999999999999995, 0.39390909090909093, 0.9145454545454544, 0.06242727272727273, 148.0), Array(0.0, 0.0, 1.0, 0.7752000000000001, 0.711524, 0.47440000000000004, 0.183296, 7609.0), Array(1.0, 0.0, 1.0, 0.5714285714285714, 0.5541142857142857, 0.52, 0.2388142857142857, 2574.0), Array(1.0, 0.0, 0.0, 0.882, 0.7954500000000001, 0.378, 0.20598, 4597.0))

// merging the numerical columns with 1-of-k vectors produced

val vect = concat.zip(rdd5).map(x => (x._1.toList ++ x._2.toList).toArray).map( x => x.map( y => y.toString.toDouble) )
vect: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[54] at map at <console>:29

vect.first.size
res4: Int = 39

vect.take(5)
res14: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.43000000000000005, 0.4091, 0.7749999999999999, 0.20895, 42.0), Array(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.37999999999999995, 0.39390909090909093, 0.9145454545454544, 0.06242727272727273, 148.0), Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7752000000000001, 0.711524, 0.47440000000000004, 0.183296, 7609.0), Array(0...


import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import scala.math._

val data = vect.map(x => {
  val arr_size = x.size - 1 
  val l = log(x(arr_size))
  val f = x.slice(0,arr_size)
  LabeledPoint(l,Vectors.dense(f))
})

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache
testSet.cache

---- MLlib Linear regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step  -> RMSE, MSE") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LinearRegressionWithSGD
    model.setIntercept(true)
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f -> %.4f, %.4f".format(numIter, step, metrics.rootMeanSquaredError, metrics.meanSquaredError))
  }
}

iterateLRwSGD(Array(10,20,50,100),Array(1, 0.1, 0.01, 0.001), trainSet, testSet)
iter, step  -> RMSE, MSE
10, 1.00000 -> 0.9574, 0.9166
10, 0.10000 -> 1.5847, 2.5114
10, 0.01000 -> 4.7572, 22.6308
10, 0.00100 -> 5.5664, 30.9847
20, 1.00000 -> 0.9144, 0.8362
20, 0.10000 -> 1.4054, 1.9752
20, 0.01000 -> 4.3629, 19.0348
20, 0.00100 -> 5.5167, 30.4340
50, 1.00000 -> 0.8928, 0.7971
50, 0.10000 -> 1.2796, 1.6375
50, 0.01000 -> 3.6925, 13.6348
50, 0.00100 -> 5.4187, 29.3626
100, 1.00000 -> 0.8897, 0.7915
100, 0.10000 -> 1.1869, 1.4088
100, 0.01000 -> 3.0960, 9.5853
100, 0.00100 -> 5.3430, 28.5475

----- Decide to scale features because variabiliaty increases even reducing step size of model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val testScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

trainScaled.cache
testScaled.cache

---- MLlib Linear regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step  -> RMSE, MSE") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LinearRegressionWithSGD
    model.setIntercept(true)
    model.optimizer.setNumIterations(numIter).setStepSize(step)
    val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f -> %.4f, %.4f".format(numIter, step, metrics.rootMeanSquaredError, metrics.meanSquaredError))
  }
}

iterateLRwSGD(Array(10,20,50,100),Array(1, 0.1, 0.01, 0.001), trainSet, testSet)
iter, step  -> RMSE, MSE
10, 1.00000 -> 0.9574, 0.9166
10, 0.10000 -> 1.5847, 2.5114
10, 0.01000 -> 4.7572, 22.6308
10, 0.00100 -> 5.5664, 30.9847
20, 1.00000 -> 0.9144, 0.8362
20, 0.10000 -> 1.4054, 1.9752
20, 0.01000 -> 4.3629, 19.0348
20, 0.00100 -> 5.5167, 30.4340
50, 1.00000 -> 0.8928, 0.7971
50, 0.10000 -> 1.2796, 1.6375
50, 0.01000 -> 3.6925, 13.6348
50, 0.00100 -> 5.4187, 29.3626
100, 1.00000 -> 0.8897, 0.7915
100, 0.10000 -> 1.1869, 1.4088
100, 0.01000 -> 3.0960, 9.5853
100, 0.00100 -> 5.3430, 28.5475