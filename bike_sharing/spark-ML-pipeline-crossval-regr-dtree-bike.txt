
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("bike/hour.csv")

df.printSchema
root
 |-- instant: integer (nullable = true)
 |-- dteday: timestamp (nullable = true)
 |-- season: integer (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: integer (nullable = true)
 |-- hr: integer (nullable = true)
 |-- holiday: integer (nullable = true)
 |-- weekday: integer (nullable = true)
 |-- workingday: integer (nullable = true)
 |-- weathersit: integer (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- casual: integer (nullable = true)
 |-- registered: integer (nullable = true)
 |-- cnt: integer (nullable = true)
 
import org.apache.spark.sql.types._

val df1 = df.drop("instant","casual","registered").
withColumn("season",'season.cast(DoubleType)).
withColumn("mnth",'mnth.cast(DoubleType)).
withColumn("hr",'hr.cast(DoubleType)).
withColumn("holiday",'holiday.cast(DoubleType)).
withColumn("weekday",'weekday.cast(DoubleType)).
withColumn("workingday",'workingday.cast(DoubleType)).
withColumn("weathersit",'weathersit.cast(DoubleType)).
withColumn("label",'cnt.cast(DoubleType))

df1.printSchema
root
 |-- dteday: timestamp (nullable = true)
 |-- season: double (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: double (nullable = true)
 |-- hr: double (nullable = true)
 |-- holiday: double (nullable = true)
 |-- weekday: double (nullable = true)
 |-- workingday: double (nullable = true)
 |-- weathersit: double (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- cnt: integer (nullable = true)
 |-- label: double (nullable = true)


import org.apache.spark.ml.feature.VectorAssembler

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("holiday","workingday","temp","atemp","hum","windspeed","season","weekday","mnth","hr"))

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(va))

val df2 = pipeline.fit(df1).transform(df1)

df2.printSchema
root
 |-- dteday: timestamp (nullable = true)
 |-- season: double (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: double (nullable = true)
 |-- hr: double (nullable = true)
 |-- holiday: double (nullable = true)
 |-- weekday: double (nullable = true)
 |-- workingday: double (nullable = true)
 |-- weathersit: double (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- cnt: integer (nullable = true)
 |-- label: double (nullable = true)
 |-- features: vector (nullable = true)
 
df2.show(10)
+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+-----+--------------------+
|             dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|cnt|label|            features|
+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+-----+--------------------+
|2011-01-01 00:00:00|   1.0|  0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.81|      0.0| 16| 16.0|[0.0,0.0,0.24,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|1.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0| 40| 40.0|[0.0,0.0,0.22,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|2.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0| 32| 32.0|[0.0,0.0,0.22,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|3.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0| 13| 13.0|[0.0,0.0,0.24,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|4.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|  1|  1.0|[0.0,0.0,0.24,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|5.0|    0.0|    6.0|       0.0|       2.0|0.24|0.2576|0.75|   0.0896|  1|  1.0|[0.0,0.0,0.24,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|6.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|  2|  2.0|[0.0,0.0,0.22,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|7.0|    0.0|    6.0|       0.0|       1.0| 0.2|0.2576|0.86|      0.0|  3|  3.0|[0.0,0.0,0.2,0.25...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|8.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|  8|  8.0|[0.0,0.0,0.24,0.2...|
|2011-01-01 00:00:00|   1.0|  0| 1.0|9.0|    0.0|    6.0|       0.0|       1.0|0.32|0.3485|0.76|      0.0| 14| 14.0|[0.0,0.0,0.32,0.3...|
+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+-----+--------------------+
only showing top 10 rows


// ----- building the decision tree model

import org.apache.spark.ml.regression.DecisionTreeRegressor
val dt = new DecisionTreeRegressor
dt.setMaxDepth(7).setMaxBins(32).setFeaturesCol("features")

import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline().setStages(Array(va,dt))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

val model = pipeline.fit(trainingData)

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.RegressionEvaluator
val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res47: Double = 92.27469283944842

bceval.setMetricName("r2").evaluate(pred)
res48: Double = 0.7457823556137954

import org.apache.spark.ml.regression.DecisionTreeRegressionModel
val dtmodel = model.stages(1).asInstanceOf[DecisionTreeRegressionModel]

dtmodel.toDebugString
res49: String =
"DecisionTreeRegressionModel (uid=dtr_3a10ff558352) of depth 7 with 255 nodes
  If (feature 9 <= 6.5)
   If (feature 9 <= 5.5)
    If (feature 9 <= 0.5)
     If (feature 1 <= 0.5)
      If (feature 2 <= 0.45)
       If (feature 3 <= 0.29545)
        If (feature 6 <= 2.5)
         Predict: 35.74193548387097
        Else (feature 6 > 2.5)
         Predict: 58.375
       Else (feature 3 > 0.29545)
        If (feature 5 <= 0.1791)
         Predict: 92.75
        Else (feature 5 > 0.1791)
         Predict: 60.40909090909091
      Else (feature 2 > 0.45)
       If (feature 8 <= 5.5)
        If (feature 5 <= 0.26865)
         Predict: 108.0
        Else (feature 5 > 0.26865)
         Predict: 29.0
       Else (feature 8 > 5.5)
        If (feature 2 <= 0.71)
         Predict: 12...

-------------------

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(dt.maxDepth, Array(7, 10, 20)).
addGrid(dt.maxBins, Array(16, 32, 48)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.DecisionTreeRegressionModel
val dtmodel = bestmodel.stages(1).asInstanceOf[DecisionTreeRegressionModel]

dtmodel.getMaxDepth
res9: Int = 10

dtmodel.getMaxBins
res10: Int = 48

val pred = bestmodel.transform(testData)

val bceval = new RegressionEvaluator()

bceval.setMetricName("rmse").evaluate(pred)
res52: Double = 82.03346810424851

bceval.setMetricName("r2").evaluate(pred)
res53: Double = 0.7990802548077033

dtmodel.toDebugString
res54: String =
"DecisionTreeRegressionModel (uid=dtr_3a10ff558352) of depth 10 with 1739 nodes
  If (feature 9 <= 6.5)
   If (feature 9 <= 5.5)
    If (feature 9 <= 0.5)
     If (feature 1 <= 0.5)
      If (feature 2 <= 0.45)
       If (feature 3 <= 0.29545)
        If (feature 6 <= 2.5)
         If (feature 2 <= 0.29000000000000004)
          If (feature 7 <= 1.5)
           If (feature 3 <= 0.26515)
            Predict: 37.2
           Else (feature 3 > 0.26515)
            Predict: 69.0
          Else (feature 7 > 1.5)
           If (feature 4 <= 0.695)
            Predict: 28.444444444444443
           Else (feature 4 > 0.695)
            Predict: 16.0
         Else (feature 2 > 0.29000000000000004)
          If (feature 4 <= 0.525)
           If (feature 3 <= 0.2803)
            P...