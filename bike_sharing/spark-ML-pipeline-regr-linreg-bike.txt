
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("bike/hour.csv")

df.printSchema
root
 |-- instant: integer (nullable = true)
 |-- dteday: timestamp (nullable = true)
 |-- season: integer (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: integer (nullable = true)
 |-- hr: integer (nullable = true)
 |-- holiday: integer (nullable = true)
 |-- weekday: integer (nullable = true)
 |-- workingday: integer (nullable = true)
 |-- weathersit: integer (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- casual: integer (nullable = true)
 |-- registered: integer (nullable = true)
 |-- cnt: integer (nullable = true)
 
import org.apache.spark.sql.types._

val df1 = df.drop("instant","casual","registered").
withColumn("season",'season.cast(DoubleType)).
withColumn("mnth",'mnth.cast(DoubleType)).
withColumn("hr",'hr.cast(DoubleType)).
withColumn("holiday",'holiday.cast(DoubleType)).
withColumn("weekday",'weekday.cast(DoubleType)).
withColumn("workingday",'workingday.cast(DoubleType)).
withColumn("weathersit",'weathersit.cast(DoubleType)).
withColumn("label",'cnt.cast(DoubleType))

df1.printSchema
root
 |-- dteday: timestamp (nullable = true)
 |-- season: double (nullable = true)
 |-- yr: integer (nullable = true)
 |-- mnth: double (nullable = true)
 |-- hr: double (nullable = true)
 |-- holiday: double (nullable = true)
 |-- weekday: double (nullable = true)
 |-- workingday: double (nullable = true)
 |-- weathersit: double (nullable = true)
 |-- temp: double (nullable = true)
 |-- atemp: double (nullable = true)
 |-- hum: double (nullable = true)
 |-- windspeed: double (nullable = true)
 |-- cnt: integer (nullable = true)
 |-- label: double (nullable = true)


import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}
val dfOne1 = new OneHotEncoder().setInputCol("season").setOutputCol("seasonVect")
val dfOne2 = new OneHotEncoder().setInputCol("mnth").setOutputCol("mnthVect")
val dfOne3 = new OneHotEncoder().setInputCol("hr").setOutputCol("hrVect")
val dfOne4 = new OneHotEncoder().setInputCol("weekday").setOutputCol("weekdayVect")

val va = new VectorAssembler().setOutputCol("features").setInputCols(Array("seasonVect","mnthVect","hrVect","weekdayVect","holiday","workingday","temp","atemp","hum","windspeed"))

import org.apache.spark.ml.feature.StandardScaler

val scaler = new StandardScaler().
setInputCol("features").
setOutputCol("scaledFeatures").
setWithStd(true).
setWithMean(true)

import org.apache.spark.ml.regression.LinearRegression

val lr = new LinearRegression().
setMaxIter(500).
setRegParam(0.1).
setFitIntercept(true).
setFeaturesCol("scaledFeatures")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(dfOne1,dfOne2,dfOne3,dfOne4,va,scaler,lr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

val model = pipeline.fit(trainingData)

val pred = model.transform(testData)

import org.apache.spark.ml.evaluation.RegressionEvaluator

val evaluator = new RegressionEvaluator().
setMetricName("rmse").
setLabelCol("label").
setPredictionCol("prediction")

evaluator.evaluate(pred)
res145: Double = 112.32305909586832

------------------------------

import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}

val paramGrid = new ParamGridBuilder().
addGrid(lr.regParam, Array(1, 0.1, 0.01, 0.001)).
addGrid(lr.fitIntercept, Array(true)).
addGrid(lr.maxIter, Array(100,200,300)).build()

import org.apache.spark.ml.evaluation.RegressionEvaluator

val cv = new CrossValidator().
setEstimator(pipeline).
setEvaluator(new RegressionEvaluator).
setEstimatorParamMaps(paramGrid).
setNumFolds(3)

val model = cv.fit(trainingData)

import org.apache.spark.ml.PipelineModel
val bestmodel = model.bestModel.asInstanceOf[PipelineModel]

import org.apache.spark.ml.regression.LinearRegressionModel
val lrmodel = bestmodel.stages(6).asInstanceOf[LinearRegressionModel]

lrmodel.getRegParam
res7: Double = 1.0

lrmodel.getMaxIter
res8: Int = 100

lrmodel.getFitIntercept
res9: Boolean = true

lrmodel.getStandardization
res10: Boolean = true

println(s"LR Model coefficients:\n${lrmodel.coefficients.toArray.mkString("\n")}")
LR Model coefficients:
0.0
-25.7131475793608
-9.247786153570484
-14.906226381189803
0.0
0.9863327672386838
-0.2303898684004245
1.2719058773694891
-1.9647206294973525
1.2122742071449601
-4.609988190547383
-10.056010005982062
-3.308907799689818
6.522446416460105
3.540440565532418
-1.1892370980924025
-7.708842455576622
-10.768844919646359
-12.356943165793153
-14.167449677191854
-14.317790606451734
-11.599620312410584
-0.032673249940157335
26.64731988973515
52.19587739388253
24.15403097379302
12.111211784882743
15.861543862460596
24.36061649322265
21.907004348323746
18.40505046096757
20.669462944665923
32.56124962983805
64.04890563969505
57.885529496128925
35.96732649246604
21.80227559176302
12.797888179052386
5.4671614326479405
-5.751608711888122
-1.5060135719527044
-1.0760285917845764
-2.067049582179042
-1.1735014332008822
1.404256370275144
-4.941742995039431
-1.5508323437280536
30.870987820800895
26.428009222329255
-30.450537854870397
-6.346885882241119

val pred = bestmodel.transform(testData)

import org.apache.spark.ml.evaluation.RegressionEvaluator

val evaluator = new RegressionEvaluator().
setMetricName("rmse").
setLabelCol("label").
setPredictionCol("prediction")

evaluator.evaluate(pred)
res37: Double = 112.35660980140659