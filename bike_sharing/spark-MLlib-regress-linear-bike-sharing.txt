---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("bike/hour.csv").filter( x => ! x.contains("instant")).map(x => x.split(","))

rdd.take(2)
res0: Array[Array[String]] = Array(Array(1, 2011-01-01, 1, 0, 1, 0, 0, 6, 0, 1, 0.24, 0.2879, 0.81, 0, 3, 13, 16), Array(2, 2011-01-01, 1, 0, 1, 1, 0, 6, 0, 1, 0.22, 0.2727, 0.8, 0, 8, 32, 40))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collectAsMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
    val categoryIdx = categories(r(idx)).toInt
	val categoryFeatures = if (numCategories > 2) Array.ofDim[Double](numCategories) else Array.ofDim[Double](1)
	if (numCategories > 2) categoryFeatures(categoryIdx) = 1.0 else categoryFeatures(0) = categoryIdx
    categoryFeatures
	})
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,2,5,7,9)

val rdd1 = rdd.map(x => Array(x(6).toDouble,x(8).toDouble,x(10).toDouble,x(11).toDouble,x(12).toDouble,x(13).toDouble,x(16).toDouble))

val vect = concat.zip(rdd1).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(2)
res1: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.24, 0.2879, 0.81, 0.0, 16.0), Array(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22, 0.2727, 0.8, 0.0, 40.0))

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val arr_size = x.size - 1
   val l = x(arr_size)-1
   val f = x.slice(0,arr_size)
   LabeledPoint(l, Vectors.dense(f))
 })

val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib Linear regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step  -> RMSE, MSE") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LinearRegressionWithSGD
	model.setIntercept(true)
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f -> %.4f, %.4f".format(numIter, step, metrics.rootMeanSquaredError, metrics.meanSquaredError))
  }
}

iterateLRwSGD(Array(100,300,500),Array(1, 0.1, 0.01, 0.001), trainSet, testSet)
iter, step  -> RMSE, MSE
100, 1.00000 -> 123.4816, 15247.7073
100, 0.10000 -> 165.6834, 27450.9898
100, 0.01000 -> 203.5409, 41428.8867
100, 0.00100 -> 251.8643, 63435.6417
300, 1.00000 -> 116.5723, 13589.0935
300, 0.10000 -> 158.3852, 25085.8685
300, 0.01000 -> 186.4213, 34752.9098
300, 0.00100 -> 245.7862, 60410.8504
500, 1.00000 -> 116.5723, 13589.0935
500, 0.10000 -> 157.9805, 24957.8342
500, 0.01000 -> 186.4213, 34752.9098
500, 0.00100 -> 242.4772, 58795.1964

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = data.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res7: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,0.8507,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]

matrixSummary.min
res8: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]

matrixSummary.mean
res9: org.apache.spark.mllib.linalg.Vector = [0.028770355026181024,0.6827205247712756,0.4969871684216565,0.47577510213476043,0.6272288394038795,0.19009760630646091,0.24351228494159619,0.25369699062086426,0.25870303239541975,0.2440876920421198,0.04010587490649634,0.04183209620806721,0.04194717762817193,0.04188963691811957,0.04188963691811957,0.04188963691811957,0.04183209620806721,0.04194717762817193,0.041256689107543584,0.04188963691811957,0.041717014787962484,0.04177455549801484,0.041141607687438866,0.04200471833822429,0.04183209620806721,0.04010587490649634,0.04188963691811957,0.04200471833822429,0.04188963691811957,0.04194717762817193,0.041659474077910125,0.04183209620806721,0.04183209620806721,0.04188963691811957,0.14218309453938663,0.1431037459002244,0.14454226365153347,0.143966856...

matrixSummary.variance
res10: org.apache.spark.mllib.linalg.Vector = [0.027944229628663223,0.216625674622443,0.03707785983073765,0.029532496613919268,0.03722192087154383,0.014967131527242529,0.1842246524414297,0.1893457226581085,0.1917868089740012,0.18451950801918102,0.03849960899931937,0.040084478424256824,0.04018992447399371,0.0401372047602491,0.0401372047602491,0.0401372047602491,0.040084478424256824,0.04018992447399371,0.039556850840711894,0.0401372047602491,0.039979005885529245,0.04003174546601687,0.0394512458570308,0.04024263756549064,0.040084478424256824,0.03849960899931937,0.0401372047602491,0.04024263756549064,0.0401372047602491,0.04018992447399371,0.03992625968279394,0.040084478424256824,0.040084478424256824,0.0401372047602491,0.12197408064179857,0.122632120148817,0.1236569129765996,0.12324749251406...

---- Apply standardization to dataset -------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))
val testScaled = testSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

trainScaled.cache

---- MLlib Linear regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step  -> RMSE, MSE") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LinearRegressionWithSGD
	model.setIntercept(true)
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f -> %.4f, %.4f".format(numIter, step, metrics.rootMeanSquaredError, metrics.meanSquaredError))
  }
}

iterateLRwSGD(Array(100,300,500),Array(1, 0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step  -> RMSE, MSE
100, 1.00000 -> 111.6785, 12472.0780
100, 0.10000 -> 116.2307, 13509.5644
100, 0.01000 -> 218.3748, 47687.5743
100, 0.00100 -> 255.3286, 65192.7186
300, 1.00000 -> 111.6785, 12472.0780
300, 0.10000 -> 114.0122, 12998.7735
300, 0.01000 -> 194.3978, 37790.5221
300, 0.00100 -> 251.5863, 63295.6718
500, 1.00000 -> 111.6785, 12472.0780
500, 0.10000 -> 114.0122, 12998.7735
500, 0.01000 -> 187.2393, 35058.5679
500, 0.00100 -> 249.4216, 62211.1252
